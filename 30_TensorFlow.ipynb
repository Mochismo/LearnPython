{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "30_TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1uZ2LifUuxcGpmht1hZ68S66y-gi7g-7n",
      "authorship_tag": "ABX9TyObnU4089bHtr0hF7Tv8l+b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mochismo/LearnPython/blob/main/30_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1\n",
        "\n",
        "From the scratch implementations:\n",
        "\n",
        "\n",
        "*   We had to initialize weights\n",
        "*   We needed an epoch loop\n",
        "*   We coded the activation functions\n",
        "*   We decided the learning rate, sizes, number of nodes and so on.\n",
        "\n"
      ],
      "metadata": {
        "id": "5dtDIij_B9gU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2"
      ],
      "metadata": {
        "id": "7Ca5XrDb3t9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "tf = tf.compat.v1\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "#Getting the Iris dataset and preparing the variables and target\n",
        "dataset_path =\"/content/drive/MyDrive/Iris.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "X \n",
        "\n",
        "# Convert labels to numbers\n",
        "y[y=='Iris-versicolor'] = 0\n",
        "y[y=='Iris-virginica'] = 1\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Further split into train and val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    # Iterator to get the mini-batch\n",
        "    \n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Configure hyperparameters\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# Determine the form of the arguments to be passed to the computational graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train's mini-batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \n",
        "    \"\"\"\n",
        "    A simple three-layer neural network\n",
        "    \"\"\"\n",
        "    # Declaring weights and biases\n",
        "\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    \n",
        "    # tf.add and + are equivalent\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        "\n",
        "# Load the network structure \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "\n",
        "# Optimization methods\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimation results\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "\n",
        "# Index value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialize the variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Run a computational graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop per epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqImw0oSuC9_",
        "outputId": "d8fde0a3-0d69-458a-ce72-20a7922a3015"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 52.0394, val_loss : 33.5849, acc : 0.250, val_acc : 0.625\n",
            "Epoch 1, loss : 10.1124, val_loss : 25.9346, acc : 0.750, val_acc : 0.375\n",
            "Epoch 2, loss : 11.2048, val_loss : 14.1081, acc : 0.750, val_acc : 0.625\n",
            "Epoch 3, loss : 1.2206, val_loss : 8.7826, acc : 0.750, val_acc : 0.438\n",
            "Epoch 4, loss : 4.8134, val_loss : 7.8843, acc : 0.750, val_acc : 0.688\n",
            "Epoch 5, loss : 0.0000, val_loss : 1.7232, acc : 1.000, val_acc : 0.750\n",
            "Epoch 6, loss : 0.0002, val_loss : 2.1494, acc : 1.000, val_acc : 0.875\n",
            "Epoch 7, loss : 0.0002, val_loss : 2.3657, acc : 1.000, val_acc : 0.875\n",
            "Epoch 8, loss : 0.0000, val_loss : 1.1446, acc : 1.000, val_acc : 0.875\n",
            "Epoch 9, loss : 1.0139, val_loss : 5.8009, acc : 0.750, val_acc : 0.688\n",
            "test_acc : 0.800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem3\n",
        "\n",
        "All three types of objective variables"
      ],
      "metadata": {
        "id": "hdjFcLPe3bqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the Iris dataset and preparing the variables and target\n",
        "dataset_path =\"/content/drive/MyDrive/Iris.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "\n",
        "# Convert labels to numbers\n",
        "y[y=='Iris-setosa'] = 0\n",
        "y[y=='Iris-versicolor'] = 1\n",
        "y[y=='Iris-virginica'] = 2\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Further split into train and val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# One-hot encoding of correct label value\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train)\n",
        "y_val_one_hot = enc.transform(y_val)\n",
        "y_test_one_hot = enc.transform(y_test)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    # Iterator to get the mini-batch\n",
        "    \n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Configure hyperparameters\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3\n",
        "\n",
        "# Determine the form of the arguments to be passed to the computational graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train's mini-batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \n",
        "    \"\"\"\n",
        "    A simple three-layer neural network\n",
        "    \"\"\"\n",
        "    # Declaring weights and biases\n",
        "    \n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        " \n",
        "# Load the network structure \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "\n",
        "# Optimization methods\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimation results\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
        "\n",
        "# Index value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialize the variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Run a computational graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop per epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb9_Xk9K0iyi",
        "outputId": "9eef5ae0-bba3-473c-81f4-44aa88b30ca5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 0.6590, val_loss : 15.6386, acc : 0.667, val_acc : 0.625\n",
            "Epoch 1, loss : 0.1329, val_loss : 3.7865, acc : 0.833, val_acc : 0.667\n",
            "Epoch 2, loss : 0.0000, val_loss : 2.1666, acc : 1.000, val_acc : 0.917\n",
            "Epoch 3, loss : 0.0077, val_loss : 4.1009, acc : 1.000, val_acc : 0.792\n",
            "Epoch 4, loss : 0.0003, val_loss : 4.1552, acc : 1.000, val_acc : 0.875\n",
            "Epoch 5, loss : 0.0000, val_loss : 2.5708, acc : 1.000, val_acc : 0.917\n",
            "Epoch 6, loss : 0.0000, val_loss : 1.4122, acc : 1.000, val_acc : 0.833\n",
            "Epoch 7, loss : 0.0000, val_loss : 4.1130, acc : 1.000, val_acc : 0.750\n",
            "Epoch 8, loss : 0.0000, val_loss : 10.1753, acc : 1.000, val_acc : 0.625\n",
            "Epoch 9, loss : 0.0000, val_loss : 6.6393, acc : 1.000, val_acc : 0.667\n",
            "test_acc : 0.933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4\n",
        "\n",
        "Model using the House Prices"
      ],
      "metadata": {
        "id": "NuRsNmT73MqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#Getting the Iris dataset and preparing the variables and target\n",
        "dataset_path =\"/content/drive/MyDrive/train.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "y = df[\"SalePrice\"]\n",
        "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "y = np.log(y)\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Further split into train and val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    # Iterator to get the mini-batch\n",
        "    \n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Configure hyperparameters\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# Determine the form of the arguments to be passed to the computational graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train's mini-batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \n",
        "    \"\"\"\n",
        "    A simple three-layer neural network\n",
        "    \"\"\"\n",
        "    # Declaring weights and biases    \n",
        "    \n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        "\n",
        "# Load the network structure \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op =  tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
        "\n",
        "# Optimization methods\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "# Estimation results\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "y_pred = logits\n",
        "\n",
        "# Initialize the variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Run a computational graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    loss_list = []\n",
        "    val_loss_list = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop per epoch\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "        loss = sess.run(loss_op, feed_dict={X: X_train, Y: y_train})\n",
        "        loss_list.append(loss)\n",
        "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
        "        val_loss_list.append(val_loss)    \n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\n",
        "    print(\"test_mse : {:.3f}\".format(loss))\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.plot(loss_list, label='loss')\n",
        "    plt.plot(val_loss_list, label='val_loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "qgp7D-ab1zmT",
        "outputId": "75e2b039-5d3c-43b8-d4fb-451fba94f244"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 622837.6250, val_loss : 531596.0000\n",
            "Epoch 1, loss : 87521.8516, val_loss : 88678.2422\n",
            "Epoch 2, loss : 55369.6836, val_loss : 54628.9375\n",
            "Epoch 3, loss : 59290.9609, val_loss : 46546.9453\n",
            "Epoch 4, loss : 34019.7656, val_loss : 37747.8789\n",
            "Epoch 5, loss : 26076.6621, val_loss : 27917.8848\n",
            "Epoch 6, loss : 22524.7344, val_loss : 23553.4609\n",
            "Epoch 7, loss : 19543.1406, val_loss : 20404.0586\n",
            "Epoch 8, loss : 16282.1084, val_loss : 17471.7910\n",
            "Epoch 9, loss : 10986.9922, val_loss : 12676.5498\n",
            "test_mse : 10986.992\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 5\n",
        "\n",
        "Model that classifies the MNIST"
      ],
      "metadata": {
        "id": "wqCSd_BD26Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the MNIST dataset\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "#Flatten\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "# Type conversion, normalization\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
        "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:])\n",
        "y_test_one_hot = enc.fit_transform(y_test[:])\n",
        "\n",
        "#Split into train and validations sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
        "\n",
        "class GetMiniBatch:\n",
        "    \n",
        "    # Iterator to get the mini-batch\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Configure hyperparameters\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "\n",
        "# Determine the form of the arguments to be passed to the computational graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train's mini-batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \n",
        "    \"\"\"\n",
        "    A simple three-layer neural network\n",
        "    \"\"\"\n",
        "    # Declaring weights and biases  \n",
        "    \n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        "\n",
        "# Load the network structure                                \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "\n",
        "# Optimization methods\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimation results\n",
        "max_Y = (tf.argmax(Y, 1))\n",
        "max_Y_pred = tf.argmax(logits, 1)\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "#initialize the variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Run a computational graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop per epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "    \n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Wu6HXyv2TgL",
        "outputId": "514df8b5-42c6-4001-b893-0cd074b1c676"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 1.7563, val_loss : 1.3115, acc : 0.700, val_acc : 0.611\n",
            "Epoch 1, loss : 1.7962, val_loss : 1.1792, acc : 0.700, val_acc : 0.605\n",
            "Epoch 2, loss : 1.1392, val_loss : 0.7899, acc : 0.700, val_acc : 0.710\n",
            "Epoch 3, loss : 0.7814, val_loss : 0.4994, acc : 0.700, val_acc : 0.869\n",
            "Epoch 4, loss : 0.8545, val_loss : 0.4140, acc : 0.700, val_acc : 0.888\n",
            "Epoch 5, loss : 0.3320, val_loss : 0.4330, acc : 0.900, val_acc : 0.893\n",
            "Epoch 6, loss : 0.9279, val_loss : 0.3793, acc : 0.800, val_acc : 0.897\n",
            "Epoch 7, loss : 0.2194, val_loss : 0.3325, acc : 0.800, val_acc : 0.915\n",
            "Epoch 8, loss : 0.5005, val_loss : 0.3536, acc : 0.800, val_acc : 0.919\n",
            "Epoch 9, loss : 0.9110, val_loss : 0.3664, acc : 0.800, val_acc : 0.913\n",
            "test_acc : 0.914\n"
          ]
        }
      ]
    }
  ]
}