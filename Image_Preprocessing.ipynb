{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Preprocessing",
      "provenance": [],
      "mount_file_id": "1dqQKRJqQwe8REhGGePaNbxK0Nviioi3U",
      "authorship_tag": "ABX9TyOV1K38baI+OxBKGidrzEtw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mochismo/LearnPython/blob/main/Image_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkTKWIBz1HaZ",
        "outputId": "a49b8870-d625-46ce-917b-7c65fe75e915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 processed..\n",
            "100 processed..\n",
            "200 processed..\n",
            "300 processed..\n",
            "400 processed..\n",
            "500 processed..\n",
            "600 processed..\n",
            "700 processed..\n",
            "800 processed..\n",
            "0 processed..\n",
            "100 processed..\n",
            "200 processed..\n",
            "300 processed..\n",
            "400 processed..\n",
            "500 processed..\n",
            "600 processed..\n",
            "700 processed..\n",
            "800 processed..\n",
            "0 processed..\n",
            "100 processed..\n",
            "200 processed..\n",
            "300 processed..\n",
            "400 processed..\n",
            "500 processed..\n",
            "600 processed..\n",
            "700 processed..\n",
            "800 processed..\n",
            "0 processed..\n",
            "100 processed..\n",
            "200 processed..\n",
            "300 processed..\n",
            "400 processed..\n",
            "500 processed..\n",
            "600 processed..\n",
            "700 processed..\n",
            "800 processed..\n",
            "0 processed..\n",
            "100 processed..\n",
            "200 processed..\n",
            "300 processed..\n",
            "400 processed..\n",
            "500 processed..\n",
            "600 processed..\n",
            "700 processed..\n",
            "800 processed..\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Nov 30 14:46:20 2020\n",
        "\n",
        "@author: scholar1\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import math\n",
        "import cv2\n",
        "import pickle\n",
        "\n",
        "# scale image initially so that crop_size covers the target feature well\n",
        "scale_ratio = 0.25 # this should be 1/3?\n",
        "\n",
        "control_point_offset = np.array([[0.05, 0, 0], [-0.05, 0, 0], [0, 0.05, 0], [0, -0.05, 0], [0, 0, 0.05], [0, 0, -0.05]])\n",
        "\n",
        "def generate_output_directories(output_dir, num_data):\n",
        "\tif os.path.exists(output_dir) == False:\n",
        "\t\tos.mkdir(output_dir)\n",
        "\n",
        "\tfor i in range(num_data):\n",
        "\t\tpart_dir = os.path.join(output_dir, str(i))\n",
        "\t\tif os.path.exists(part_dir) == False:\n",
        "\t\t\tos.mkdir(part_dir)\n",
        "            \n",
        "    #marker points for backgound (refference or zero point)\n",
        "\tbackground_dir = os.path.join(output_dir, \"background\")\n",
        "\tif os.path.exists(background_dir) == False:\n",
        "\t\tos.mkdir(background_dir)\n",
        "\n",
        "\tfor i in range(num_data):\n",
        "\t\tpart_dir = os.path.join(output_dir, \"64x64_\" + str(i))\n",
        "\t\tif os.path.exists(part_dir) == False:\n",
        "\t\t\tos.mkdir(part_dir)\n",
        "\n",
        "\n",
        "def read_poses(path_to_data):\n",
        "\tfile_name = os.path.join(data_dir, \"poseGT.txt\")\n",
        "\n",
        "\tf = open(file_name, 'r')\n",
        "\n",
        "\tdict = {}\n",
        "\n",
        "\twith f as open_file_object:\n",
        "\t\tfor line in open_file_object:\n",
        "\t\t\telements = line.split()\n",
        "\t\t\tfloats = [float(x) for x in elements[1:]]\n",
        "\t\t\tdict[elements[0]] = floats\n",
        "\n",
        "\treturn dict\n",
        "\n",
        "def read_point_file(file_path):\n",
        "\tpoints = []\n",
        "\twith open(file_path) as f:\n",
        "\t\tcontent = f.readlines()\n",
        "\n",
        "\t\tfor line in content:\n",
        "\t\t\telements = line.split()\n",
        "\t\t\tpoints.append([float(x) for x in elements])\n",
        "        \n",
        "\treturn points\n",
        "       \n",
        "def clamp(x, min_val, max_val):\n",
        "\treturn min(max(x, min_val), max_val)\n",
        "\n",
        "def difference_of_Gaussians(img):\n",
        "\t#run a 5x5 gaussian blur then a 3x3 gaussian blr\n",
        "\tkernel1 = cv2.getGaussianKernel(10, 1)\n",
        "\tkernel2 = cv2.getGaussianKernel(10, 3)\n",
        "\n",
        "\tdog_img = cv2.sepFilter2D(img, -1, kernel1 - kernel2, kernel1 - kernel2)\n",
        "\tdog_img = cv2.normalize(dog_img, dog_img, alpha = 0, beta = 255, norm_type=cv2.NORM_MINMAX)\n",
        "\n",
        "\treturn dog_img\n",
        "\n",
        "\n",
        "def output_cropped_image(x, y, img, output_path, crop_size, apply_dog = False):\n",
        "\thalf_crop_size = int(crop_size / 2)\n",
        "\tx_min = x - half_crop_size\n",
        "\tx_max = x + half_crop_size\n",
        "\ty_min = y - half_crop_size\n",
        "\ty_max = y + half_crop_size\n",
        "\tif x_min < 0 or x_max >= np.size(img,1) or y_min < 0 or y_max >= np.size(img,0):\n",
        "\t\treturn False\n",
        "\n",
        "\tcrop = img[y_min:y_max, x_min:x_max]\n",
        "\tif apply_dog:\n",
        "\t\tcrop = difference_of_Gaussians(crop)\n",
        "\n",
        "\tif crop.shape != (crop_size, crop_size):\n",
        "\t\tprint(\"crop.shape\", crop.shape, \" something is wrong..\")\n",
        "\t\tprint(x_min, x_max, y_min, y_max)\n",
        "\t\tsys.exit(1)\n",
        "\n",
        "\tcv2.imwrite(output_path, crop)\n",
        "\n",
        "\treturn True\n",
        "\n",
        "def write_part_detection_training_img(img, i, annotation_projected_points, data_id, output_dir):\n",
        "\tcrop_size = 32\n",
        "\n",
        "\tfor j in range(len(annotation_projected_points)):\n",
        "\t\tprojected_point = (annotation_projected_points[j] * scale_ratio).astype(int)\n",
        "\t\t#print(annotation_projected_points[j], projected_point)\n",
        "\t\toutput_path = os.path.join(output_dir, str(j), str(data_id) + \"_\" + str(i).zfill(5) + \".png\")\n",
        "\t\toutput_cropped_image(projected_point[0][0], projected_point[0][1], img, output_path, crop_size, True)\n",
        "\n",
        "\t# generate background example\n",
        "\twhile True:\n",
        "\t\thalf_crop_size = int(crop_size / 2)\n",
        "\t\tx = np.random.randint(half_crop_size, np.size(img,1)-half_crop_size)\n",
        "\t\ty = np.random.randint(half_crop_size, np.size(img,0)-half_crop_size)\n",
        "\t\tclose_to_annotated_points = False\n",
        "\t\tfor j in range(len(annotation_projected_points)):\n",
        "\t\t\tprojected_point = (annotation_projected_points[j] * scale_ratio).astype(int)\n",
        "\t\t\tif abs(x - projected_point[0][0]) <= half_crop_size or abs(y - projected_point[0][1]) <= half_crop_size:\n",
        "\t\t\t\tclose_to_annotated_points = True\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\tif close_to_annotated_points == False:\n",
        "\t\t\t# region not containing any annotated part is found\n",
        "\t\t\toutput_path = os.path.join(output_dir, \"background\", str(data_id) + \"_\" + str(i).zfill(5) + \".png\")\n",
        "\t\t\toutput_cropped_image(x, y, img, output_path, crop_size, True)\n",
        "\t\t\tbreak\n",
        "\n",
        "def write_control_points_detection_training_img(img, i, annotation_projected_points, data_id, output_dir):\n",
        "\tcrop_size = 64\n",
        "\tis_cropped_image_saved = []\n",
        "\n",
        "\tfor j in range(len(annotation_projected_points)):\n",
        "\t\tprojected_point = (annotation_projected_points[j] * scale_ratio).astype(int)\n",
        "\t\t#print(annotation_projected_points[j], projected_point)\n",
        "\t\toutput_path = os.path.join(output_dir, \"64x64_\" + str(j), str(data_id) + \"_\" + str(i).zfill(5) + \".png\")\n",
        "\t\tsuccess = output_cropped_image(projected_point[0][0], projected_point[0][1], img, output_path, crop_size)\n",
        "\t\tis_cropped_image_saved.append(success)\n",
        "\n",
        "\treturn is_cropped_image_saved\n",
        "\n",
        "\n",
        "def extract_images(data_id, data_dir, output_dir, all_projected_control_points):\n",
        "\tvisualization = False\n",
        "\n",
        "\n",
        "    \n",
        "\tpath_to_data = os.path.join(data_dir, \"frame0000\" + str(data_id))\n",
        "    #\n",
        "\n",
        "\tif visualization == True:\n",
        "\t\tmarker_3d_points = np.array(read_point_file(os.path.join(path_to_data, \"markers3dPoints.txt\")))\n",
        "\t\tmarker_3d_points.reshape(-1, 3)\n",
        "\n",
        "\tcontrol_points_for_all_parts = []\n",
        "\tfor annotation_pt in annotation_3d_points:\n",
        "\t\tcontrol_pts_for_part = []\n",
        "\t\tfor offset in control_point_offset:\n",
        "\t\t\tcontrol_pts_for_part.append(annotation_pt + offset)\n",
        "\t\t\n",
        "\t\tcontrol_points_for_all_parts.append(control_pts_for_part)\n",
        "\n",
        "\tcontrol_points_for_all_parts = np.array(control_points_for_all_parts)\n",
        "\t#assert control_points_for_all_parts.shape == (4, 6, 3)\n",
        "\n",
        "   \n",
        "\n",
        "\tRts = read_poses(path_to_data)\n",
        "\tcamera_matrix = np.float32([[2666.67, 0, 960], [0, 2666.67, 540], [0, 0, 1.]])\n",
        "    \n",
        "\n",
        "\t#index = 0\n",
        "\n",
        "\tfor i in range(0, len(Rts)):\n",
        "\t\tfile_name = \"frame\" + str(i).zfill(5) + \".png\"\n",
        "\t\timg_path = os.path.join(data_dir, file_name)\n",
        "\t\timg = cv2.imread(img_path)\n",
        "\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\t\timg = cv2.resize(img, None, fx=scale_ratio, fy=scale_ratio, interpolation=cv2.INTER_AREA) \n",
        "        \n",
        "\n",
        "\t\tRt = Rts[file_name]\n",
        "\n",
        "\t\tif math.isnan(Rt[0]) == True:\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\tannotation_projected_points, jac = cv2.projectPoints(annotation_3d_points, np.array(Rt[0:3]), np.array(Rt[3:6]), camera_matrix, np.float32([[0, 0, 0, 0, 0]]))\n",
        "#\t\tassert annotation_projected_points.shape == (4, 1, 2)\n",
        "        \n",
        "\t\twrite_part_detection_training_img(img, i, annotation_projected_points, data_id, output_dir)\n",
        "\n",
        "\t\tis_cropped_image_saved = write_control_points_detection_training_img(img, i, annotation_projected_points, data_id, output_dir)\n",
        "\n",
        "\t\tfor j in range(control_points_for_all_parts.shape[0]):\n",
        "\t\t\tif is_cropped_image_saved[j] == False:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\tprojected_control_points_for_part, jac = cv2.projectPoints(control_points_for_all_parts[j], np.array(Rt[0:3]), np.array(Rt[3:6]), camera_matrix, np.float32([[0, 0, 0, 0, 0]]))\n",
        "           # assert projected_control_points_for_part.shape == (6, 1, 2)\n",
        "\n",
        "\t\t\t# set relative offset from annotation_projected_points[j] here\n",
        "\t\t\tall_projected_control_points[j].append(projected_control_points_for_part - annotation_projected_points[j])\n",
        "\n",
        "\t\t# debug\n",
        "\t\tif visualization:\n",
        "\t\t\tcolor_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "\t\t\tfor part_id, projected_control_points_for_part in enumerate(all_projected_control_points):\n",
        "\t\t\t\tfor idx, point in enumerate(projected_control_points_for_part[-1]):\n",
        "\t\t\t\t\tcolor_code = idx % 6\n",
        "\t\t\t\t\tcolor = (0, 0, 255)\n",
        "\t\t\t\t\tif color_code == 2 or color_code == 3:\n",
        "\t\t\t\t\t\tcolor = (0, 255, 0)\n",
        "\t\t\t\t\tif color_code == 4 or color_code == 5:\n",
        "\t\t\t\t\t\tcolor = (255, 0, 0)\n",
        "\n",
        "\t\t\t\t\tpoint = (annotation_projected_points[part_id] + point) * scale_ratio\n",
        "\t\t\t\t\tcv2.circle(color_img, (int(point[0][0]), int(point[0][1])), 5, color, 1)\n",
        "\n",
        "\t\t\t\tcv2.putText(color_img, str(part_id), (int(point[0][0]), int(point[0][1])),\n",
        "\t\t\t\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "\t\t\tcv2.imwrite(os.path.join(\"debug\", str(i) + \".png\"), color_img)\n",
        "\n",
        "\t\tif i%100 == 0:\n",
        "\t\t\tprint(i, \"processed..\", flush=True)\n",
        "\n",
        "\t\tif False:\n",
        "\t\t\tfor point in annotation_projected_points:\n",
        "\t\t\t\tprint(\"point.shape\", point.shape)\n",
        "\t\t\t\tcv2.circle(img, (int(point[0][0]), int(point[0][1])), 10, (0, 255, 0), 2)\n",
        "\n",
        "\t\t\tmarker_projected_points, jac = cv2.projectPoints(marker_3d_points, np.array(Rt[0:3]), np.array(Rt[3:6]), camera_matrix, np.empty(0))\n",
        "\t\t\tfor point in marker_projected_points:\n",
        "\t\t\t\tprint(\"point.shape\", point.shape)\n",
        "\t\t\t\tcv2.circle(img, (int(point[0][0]), int(point[0][1])), 10, (0, 100, 255), 2)\n",
        "\n",
        "\t\t\tcv2.imwrite('out.png',img)\n",
        "\t\t\tcv2.imshow('image',img)\n",
        "\t\t\tk = cv2.waitKey(0)\n",
        "\t\t\tif k == 27:         # wait for ESC key to exit\n",
        "\t\t\t    cv2.destroyAllWindows()\n",
        "\t\t\t    sys.exit(0)\n",
        "\t\t\t# elif k == ord('s'): # wait for 's' key to save and exit\n",
        "\t\t\t#     cv2.imwrite('messigray.png',img)\n",
        "\t\t\t#     cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "\n",
        "#if len(sys.argv) != 2:\n",
        "#\tprint(\"usage: \" + sys.argv[0] + \" data_dir\")\n",
        "#\tsys.exit(1)\n",
        "\n",
        "#data_dir = sys.argv[1]\n",
        "data_dir = \"/content/drive/MyDrive/video1/\"\n",
        "\n",
        "annotation_3d_points = np.array(read_point_file(os.path.join(data_dir, \"markers3dPoints.txt\")))\n",
        "\n",
        "#annotation_3d_points = np.array(read_point_file(os.path.join(data_dir, \"C:\\Users\\scholar1\\Desktop\\swaymotion\\video1\\markers3dPoints.txt\")))\n",
        "annotation_3d_points.reshape(-1, 3)\n",
        "\n",
        "# extract only 4 representative parts\n",
        "annotation_3d_points = np.delete(annotation_3d_points, [1, 3, 5, 7], 0)\n",
        "assert annotation_3d_points.shape == (16, 3)\n",
        "\n",
        "output_dir = os.path.join(data_dir, \"training\")\n",
        "generate_output_directories(output_dir, len(annotation_3d_points))\n",
        "\n",
        "\n",
        "all_projected_control_points = []\n",
        "for i in range(len(annotation_3d_points)):\n",
        "\tall_projected_control_points.append([])\n",
        "\n",
        "for data_id in range(1,6):\n",
        "\textract_images(data_id, data_dir, output_dir, all_projected_control_points)\n",
        "\n",
        "\n",
        "for part_id, projected_control_points_for_part in enumerate(all_projected_control_points):\n",
        "\tfile_name = \"projected_control_points_\" + str(part_id) + \".data\"\n",
        "\twith open(os.path.join(data_dir, \"training\", file_name), 'wb') as filehandle:\n",
        "\t\t# store the data as binary data stream\n",
        "\n",
        "\t\tpickle.dump(projected_control_points_for_part, filehandle)"
      ]
    }
  ]
}