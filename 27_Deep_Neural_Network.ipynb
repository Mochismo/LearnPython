{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "27_Deep Neural Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1VI1puzoHDKzS95bwjtR_xVpgiYFe5S37",
      "authorship_tag": "ABX9TyNADf8ApzyqC/SCHeyBn8VK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mochismo/LearnPython/blob/main/27_Deep_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1 - Fully Connected Layers "
      ],
      "metadata": {
        "id": "eLTL_n7YjQtq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "p0kFuQnhgHvg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FC:\n",
        "  def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "    self.n_nodes1 = n_nodes1\n",
        "    self.n_nodes2 = n_nodes2\n",
        "    self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
        "    self.B = initializer.B(self.n_nodes2)\n",
        "    self.optimizer = optimizer\n",
        "    self.HW = 0\n",
        "    self.HB = 0\n",
        "\n",
        "  def forward(self, X):\n",
        "      self.Z = X\n",
        "      self.A = X @ self.W + self.B\n",
        "      return self.A \n",
        "\n",
        "  def backward(self, dA):\n",
        "      self.dB = np.sum(dA, axis=0)\n",
        "      self.dW  = self.Z.T @ dA\n",
        "      self.dZ = dA @ self.W.T\n",
        "      self = self.optimizer.update(self)\n",
        "      return self.dZ    "
      ],
      "metadata": {
        "id": "nuCAz5jZm88k"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2"
      ],
      "metadata": {
        "id": "o999ymNCq9jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    Simple initialization with Gaussian distribution\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      Standard deviation of Gaussian distribution\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "        Returns\n",
        "        ----------\n",
        "        W :\n",
        "        \"\"\"\n",
        "        W = self.sigma* np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "        Returns\n",
        "        ----------\n",
        "        B :\n",
        "        \"\"\"\n",
        "        B = self.sigma* np.random.randn(1, n_nodes2)\n",
        "        return B"
      ],
      "metadata": {
        "id": "91ikcRHivG4f"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem 3 - Optimization Methods"
      ],
      "metadata": {
        "id": "v8i0FP7TwJU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "      layer.W -= self.lr* layer.dW/len(layer.z)\n",
        "      layer.B -= self.lr* layer.dB/len(layer.z)\n",
        "      return layer\n",
        "    \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "uB_cgLY5wSQz"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4 - Activation Functions"
      ],
      "metadata": {
        "id": "kVX7oBd_xl4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid:\n",
        "  \n",
        "  def forward(self, A):\n",
        "    self.A = A\n",
        "    Z = 1/(1 + np.exp(-self.A))\n",
        "    return Z\n",
        "  def backward(self, dZ):\n",
        "    dA = dZ*((1/(1 + np.exp(-self.A))) - (1/(1 + np.exp(-self.A)))**2)\n",
        "    return dA"
      ],
      "metadata": {
        "id": "unEk_eg5xtQm"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh:\n",
        "  \n",
        "  def forward(self, A):\n",
        "    Z = np.tanh(self.A)\n",
        "    return Z\n",
        "  def backward(self, dZ):\n",
        "    dA = dz*(1 - np.tanh(self.A)**2)\n",
        "    return dA"
      ],
      "metadata": {
        "id": "bvuBGI491P_m"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class softmax:\n",
        "\n",
        "  def forward(self, A):\n",
        "    Z = np.exp(A) / np.sum(np.exp(A), axis=1).reshape(-1, 1)\n",
        "    return Z\n",
        "  \n",
        "  def backward(self, Z, y):\n",
        "    dA = Z -y \n",
        "    loss = - np.sum(y*np.log(Z)) / len(y)\n",
        "    return dA, loss\n"
      ],
      "metadata": {
        "id": "8iydHnvO2KgY"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 5 - Creation of the ReLU Class"
      ],
      "metadata": {
        "id": "bk87kvQc67og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU:\n",
        "\n",
        "  def forward(self, A): \n",
        "    self.A = A\n",
        "    Z = np.maximum(0, A)\n",
        "    return Z \n",
        "  \n",
        "  def backward(self, dZ):\n",
        "    dA = dZ * np.where(self.A > 0, 1, 0)\n",
        "    return dA "
      ],
      "metadata": {
        "id": "olezBeBT7KOG"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 6 - Initial value of weight"
      ],
      "metadata": {
        "id": "WUfhZjAl9d3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial value of Xavier"
      ],
      "metadata": {
        "id": "LAKEO0XP-vlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class XavierInitializer:\n",
        "\n",
        "  def __init__(self, sigma):\n",
        "    _ = sigma\n",
        "\n",
        "  def W(self, n_nodes1, n_nodes2):\n",
        "    self.sigma = 1 / np.sqt(n_nodes1)\n",
        "    W = self.sigma*np.random.randn(n_nodes1, n_nodes2)\n",
        "    return W \n",
        "\n",
        "  def W(sel, n_nodes2):\n",
        "    B = self.sigma*np.random.randn(1, n_nodes2)\n",
        "    return B \n"
      ],
      "metadata": {
        "id": "vjnGnqwB93a5"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HeInitializer:\n",
        "\n",
        "  def __init__(self, sigma):\n",
        "    _ = sigma\n",
        "\n",
        "  def W(self, n_nodes1, n_nodes2):\n",
        "    self.sigma = np.sqrt(2 / n_nodes1)\n",
        "    W = self.sigma*np.random.randn(n_nodes1, n_nodes2)\n",
        "    return W \n",
        "\n",
        "  def B(self, n_nodes2):\n",
        "    B = self.sigma*np.random.randn(1, n_nodes2)\n",
        "    return B "
      ],
      "metadata": {
        "id": "VcurYrE-BFL-"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 7 - Optimization Method"
      ],
      "metadata": {
        "id": "YQCIADpzCKH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaGrad:\n",
        "\n",
        "  def __init__(self, lr):\n",
        "    self.lr = lr \n",
        "\n",
        "  def update(self, layer):\n",
        "    layer.HW += layer.dW * layer.dW\n",
        "    layer.HB += layer.dB * layer.dB\n",
        "    delta = 1e-7\n",
        "    layer.W -=self.lr * layer.dW / (np.sqrt(layer.HW) + delta) / len(layer.Z)\n",
        "    layer.B -=self.lr * layer.dB / (np.sqrt(layer.HB) + delta) / len(layer.Z)\n",
        "    return layer "
      ],
      "metadata": {
        "id": "l5DsMeljCl9w"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 8 - Class Completion\n",
        "\n",
        "Let's complete the ScratchDeepNeuralNetrwokClassifier class that can be trained and estimated with any configuration. "
      ],
      "metadata": {
        "id": "5lOJCHQCEjv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "\n",
        "  def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "    self.batch_size = batch_size\n",
        "    np.random.seed(seed)\n",
        "    shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "    self._X = X[shuffle_index]\n",
        "    self._y = y[shuffle_index]\n",
        "    self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self._stop\n",
        "\n",
        "  def __getitem__(self, item): \n",
        "    p0 = item*self.batch_size\n",
        "    p1 = item*self.batch_size + self.batch_size\n",
        "    return self._X[p0:p1], self._y[p0:p1]\n",
        "\n",
        "  def __iter__(self): \n",
        "    self._counter = 0\n",
        "    return self \n",
        "\n",
        "  def __next__(self):\n",
        "    if self._counter >= self._stop:\n",
        "      raise StopIteration()\n",
        "    p0 = self._counter*self.batch_size\n",
        "    p1 = self._counter*self.batch_size + self.batch_size \n",
        "    self._counter += 1\n",
        "    return self._X[p0:p1], self._y[p0:p1] \n"
      ],
      "metadata": {
        "id": "tdY6SXGfGc_h"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchDeepNeuralNetrowkClassifier():\n",
        "\n",
        "  def __init__(self, verbose=False, epoch=1, optimizer= SGD, initializer = HeInitializer, activater = ReLU):\n",
        "    self.verbose = verbose\n",
        "    self.batch_size = 20 \n",
        "    self.n_features = 784\n",
        "    self.n_nodes1 = 400\n",
        "    self.n_nodes2 = 200\n",
        "    self.n_output = 10\n",
        "    self.sigma = 0.02\n",
        "    self.lr = 0.5 \n",
        "    self.epoch = epoch\n",
        "    self.optimizer = optimizer\n",
        "    self.initializer = initializer\n",
        "    self.activater = activater\n",
        "\n",
        "  def fit(self, X, y, X_val = None, y_val = None):\n",
        "    self.loss_train = []\n",
        "    self.loss_val = []\n",
        "    optimizer = self.optimizer(self.lr)\n",
        "    self.FC1 = FC(self.n_features, self.n_nodes1, self.initializer(self.sigma), optimizer)\n",
        "    self.activation1 = self.activater()\n",
        "    self.FC2 = FC(self.n_nodes1, self.n_nodes2, self.initializer(self.sigma), optimizer)\n",
        "    self.activation2 = self.activater()\n",
        "    self.FC3 = FC(self.n_nodes2, self.n_output, self.initializer(self.sigma), optimizer)\n",
        "    self.activation3 = softmax()\n",
        "\n",
        "    for i in range(self.epoch):\n",
        "      get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=i)\n",
        "      for mini_X, mini_y in get_mini_batch:\n",
        "        A1 = self.FC1.forward(mini_X)\n",
        "        Z1 = self.activation1.forward(A1)\n",
        "        A2 = self.FC2.forward(Z1)\n",
        "        Z2 = self.activation2.forward(A2)\n",
        "        # print(Z2.shape)\n",
        "\n",
        "        A3 = self.FC3.forward(Z2)\n",
        "        Z3 = self.activation3.forward(A3)\n",
        "        dA3, loss = self.activation3.backward(Z3, mini_y)\n",
        "        dZ2 = self.FC3.backward(dA3)\n",
        "        dA2 = self.activation2.backward(dZ2)\n",
        "        dZ1 = self.FC2.backward(dA2)\n",
        "        dA1 = self.activation1.backward(dZ1)\n",
        "        dZ0 = self.FC2.backward(dA1)\n",
        "\n",
        "      if self.verbose:\n",
        "        A1 = self.FC1.forward(X)\n",
        "        Z1 = self.activation1.forward(A1)\n",
        "        A2 = self.FC2.forward(Z1)\n",
        "        Z2 = self.activation2.forward(A2)\n",
        "        A3 = self.FC3.forward(Z2)\n",
        "        Z3 = self.activation3.forward(A3)\n",
        "        self.loss_train.append(self.activation3.backward(Z3, y)[1])\n",
        "\n",
        "        if X_val is not None:\n",
        "          A1 = self.FC1.forward(X_val)\n",
        "          Z1 = self.activation1.forward(A1)\n",
        "          A2 = self.FC2.forward(Z1)\n",
        "          Z2 = self.activation2.forward(A2)\n",
        "          A3 = self.FC3.forward(Z2)\n",
        "          Z3 = self.activation3.forward(A3)\n",
        "          self.loss_val.append(self.activation3.backward(Z3, y_val)[1])\n",
        "\n",
        "  def predict(sel, X):\n",
        "    A1 = self.FC1.forward(X)\n",
        "    Z1 = self.activation1.forward(A1)\n",
        "    A2 = self.FC2.forward(Z1)\n",
        "    Z2 = self.activation2.forward(A2)\n",
        "    A3 = self.FC3.forward(Z2)\n",
        "    Z3 = self.activation3.forward(A3)\n",
        "    return np.argmax(Z3, axis=1)\n",
        "\n",
        "\n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "axE4PA5nMEVJ"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 9 - Learning Estimation\n",
        "\n",
        "Let's create several networks with varying numbers of layers and activation functions."
      ],
      "metadata": {
        "id": "IyOGs0EmtFyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "metadata": {
        "id": "-yiHJsyFumEy"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
      ],
      "metadata": {
        "id": "mlb_VpVKvoq9"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown = 'ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_val[:, np.newaxis]) "
      ],
      "metadata": {
        "id": "1-9EwM1TwaI1"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SDNN = ScratchDeepNeuralNetrowkClassifier(verbose=True, epoch=10, optimizer= AdaGrad, initializer = HeInitializer, activater = ReLU)\n",
        "\n",
        "SDNN.fit(X_train, y_train_one_hot, X_val, y_test_one_hot) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "HMMhZiQaxrpK",
        "outputId": "fc359408-e72f-476e-eeaa-39b8d9c11113"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-152-f4335a45dd30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mSDNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScratchDeepNeuralNetrowkClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mAdaGrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHeInitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivater\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReLU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mSDNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-148-9d54588592d1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mdZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mdA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mdZ0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-135-e0279ce363da>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dA)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdW\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mdA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdA\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m       \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 200 is different from 400)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = SDNN.predict(X_val)\n",
        "accuracy_score(y_val, pred)"
      ],
      "metadata": {
        "id": "iuKNDskEy-Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(list(range(1, SDNN.epoch+1)), SDNN.loss_train, label='train')\n",
        "plt.plot(list(range(1, SDNN.epoch+1)), SDNN.loss_test, label='test')\n",
        "plt.legend()\n",
        "plt.xticks(list(range(1, SDNN.epoch+1)));"
      ],
      "metadata": {
        "id": "NANLHt_xz2r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the accuracy with other layers, (4 and 6 layers)"
      ],
      "metadata": {
        "id": "-xSu2yiu1Iw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchDeepNeuralNetrowkClassifier_4():\n",
        "\n",
        "  def __init__(self, verbose=False, epoch=1, optimizer= SGD, initializer = HeInitializer, activater = ReLU):\n",
        "     self.verbose = verbose\n",
        "        self.batch_size = 20\n",
        "        self.n_features = 784\n",
        "        self.n_nodes1 = 400\n",
        "        self.n_nodes2 = 200\n",
        "        self.n_nodes3 = 150\n",
        "        self.n_output = 10\n",
        "        self.sigma = 0.02\n",
        "        self.lr = 0.5\n",
        "        self.epoch = epoch\n",
        "        self.optimizer = optimizer\n",
        "        self.initializer = initializer\n",
        "        self.activater = activater\n",
        "\n",
        "  def fit(self, X, y, X_val=None, y_val=None):\n",
        "    self.loss_train = []\n",
        "    self.loss_val = []\n",
        "    optimizer = self.optimizer(self.lr)\n",
        "\n",
        "    self.FC1 = FC(self.n_features, self.n_nodes1, self.initializer(self.sigma), optimizer)\n",
        "    self.activation1 = self.activater()\n",
        "    self.FC2 = FC(self.n_nodes1, self.n_nodes2, self.initializer(self.sigma), optimizer)\n",
        "    self.activation2 = self.activater()\n",
        "    self.FC3 = FC(self.n_nodes2, self.n_nodes3, self.initializer(self.sigma), optimizer)\n",
        "    self.activation3 = self.activater()\n",
        "    self.FC4 = FC(self.n_nodes3, self.n_output, self.initializer(self.sigma), optimizer)\n",
        "    self.activation4 = softmax()\n",
        "\n",
        "    for _ in range(self.epoch):\n",
        "      get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=i)\n",
        "        for mini_X, mini_y in get_mini_batch:\n",
        "          self.forward(mini_X)\n",
        "          self.backward(mini_y)\n",
        "\n",
        "        if self.verbose:\n",
        "          self.forward(X)\n",
        "          self.loss_train.append(self.activation4.backward(self.Z4, y)[1])\n",
        "\n",
        "        if X_val is not None:\n",
        "          self.forward(X_val)\n",
        "          self.loss_val.append(self.activation4.backward(self.Z4, y_val)[1])\n",
        "\n",
        "  def forward(self, X):\n",
        "    A1 = self.FC1.forward(X)\n",
        "    Z1 = self.activation1.forward(A1)    \n",
        "    A2 = self.FC2.forward(Z1)\n",
        "    Z2 = self.activation2.forward(A2) \n",
        "    A3 = self.FC3.forward(Z2)\n",
        "    Z3 = self.activation3.forward(A3)       \n",
        "    A4 = self.FC4.forward(Z3)\n",
        "    self.Z4 = self.activation4.forward(A4)\n",
        "\n",
        "  def backward(self, y):\n",
        "    dA4, self.loss = self.activation4.backward(self.Z4, y)\n",
        "    dZ3 = self.FC4.backward(dA4) \n",
        "    dA3 = self.activation3.backward(dZ3)\n",
        "    dZ2 = self.FC3.backward(dA3) \n",
        "    dA2 = self.activation2.backward(dZ2)\n",
        "    dZ1 = self.FC2.backward(dA2) \n",
        "    dA1 = self.activation1.backward(dZ1)\n",
        "    dZ0 = self.FC1.backward(dA1) \n",
        "\n",
        "  def predict(self, X):\n",
        "    self.forward(X)\n",
        "    return np.argmax(self.Z4, axis=1)\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "      \n"
      ],
      "metadata": {
        "id": "4MkjSV7f1k-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SDNN = ScratchDeepNeuralNetrowkClassifier_4(verbose=True, epoch=10, optimizer= AdaGrad, initializer = HeInitializer, activater = ReLU)\n",
        "\n",
        "SDNN.fit(X_train, y_train_one_hot, X_val, y_test_one_hot) \n",
        "\n",
        "pred = SDNN4.predict(X_val)\n",
        "accuracy_score(y_val, pred)"
      ],
      "metadata": {
        "id": "qug_HldWKtjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(list(range(1, SDNN4.epoch+1)), SDNN4.loss_train, label='train')\n",
        "plt.plot(list(range(1, SDNN4.epoch+1)), SDNN4.loss_test, label='test')\n",
        "plt.legend()\n",
        "plt.xticks(list(range(1, SDNN4.epoch+1)));"
      ],
      "metadata": {
        "id": "TBiPWOOJLdki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchDeepNeuralNetrowkClassifier_6():\n",
        "\n",
        "  def __init__(self, verbose=False, epoch=1, optimizer= SGD, initializer = HeInitializer, activater = ReLU):\n",
        "     self.verbose = verbose\n",
        "        self.batch_size = 20\n",
        "        self.n_features = 784\n",
        "        self.n_nodes1 = 400\n",
        "        self.n_nodes2 = 200\n",
        "        self.n_nodes3 = 150\n",
        "        self.n_nodes4 = 100\n",
        "        self.n_nodes5 = 50\n",
        "        self.n_output = 10\n",
        "        self.sigma = 0.02\n",
        "        self.lr = 0.5\n",
        "        self.epoch = epoch\n",
        "        self.optimizer = optimizer\n",
        "        self.initializer = initializer\n",
        "        self.activater = activater\n",
        "\n",
        "  def fit(self, X, y, X_val=None, y_val=None):\n",
        "    self.loss_train = []\n",
        "    self.loss_val = []\n",
        "    optimizer = self.optimizer(self.lr)\n",
        "\n",
        "    self.FC1 = FC(self.n_features, self.n_nodes1, self.initializer(self.sigma), optimizer)\n",
        "    self.activation1 = self.activater()\n",
        "    self.FC2 = FC(self.n_nodes1, self.n_nodes2, self.initializer(self.sigma), optimizer)\n",
        "    self.activation2 = self.activater()\n",
        "    self.FC3 = FC(self.n_nodes2, self.n_nodes3, self.initializer(self.sigma), optimizer)\n",
        "    self.activation3 = self.activater()\n",
        "    self.FC4 = FC(self.n_nodes3, self.n_nodes4, self.initializer(self.sigma), optimizer)\n",
        "    self.activation4 = self.activater()\n",
        "    self.FC5 = FC(self.n_nodes4, self.n_nodes5, self.initializer(self.sigma), optimizer)\n",
        "    self.activation5 = self.activater()\n",
        "    self.FC6 = FC(self.n_nodes5, self.n_output, self.initializer(self.sigma), optimizer)\n",
        "    self.activation6 = softmax()\n",
        "\n",
        "    for in range(self.epoch):\n",
        "      get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=i)\n",
        "        for mini_X, mini_y in get_mini_batch:\n",
        "          self.forward(mini_X)\n",
        "          self.backward(mini_y)\n",
        "\n",
        "        if self.verbose:\n",
        "          self.forward(X)\n",
        "          self.loss_train.append(self.activation4.backward(self.Z6, y)[1])\n",
        "\n",
        "        if X_val is not None:\n",
        "          self.forward(X_val)\n",
        "          self.loss_val.append(self.activation4.backward(self.Z6, y_val)[1])\n",
        "\n",
        "  def forward(self, X):\n",
        "    A1 = self.FC1.forward(X)\n",
        "    Z1 = self.activation1.forward(A1)    \n",
        "    A2 = self.FC2.forward(Z1)\n",
        "    Z2 = self.activation2.forward(A2) \n",
        "    A3 = self.FC3.forward(Z2)\n",
        "    Z3 = self.activation3.forward(A3) \n",
        "    A4 = self.FC4.forward(Z3)\n",
        "    Z4 = self.activation4.forward(A4) \n",
        "    A5 = self.FC5.forward(Z4)\n",
        "    Z5 = self.activation5.forward(A5)             \n",
        "    A6 = self.FC6.forward(Z5)\n",
        "    self.Z6 = self.activation6.forward(A6)\n",
        "\n",
        "  def backward(self, y):\n",
        "    dA6, self.loss = self.activation6.backward(self.Z6, y)\n",
        "    dZ5 = self.FC6.backward(dA6) \n",
        "    dA5 = self.activation5.backward(dZ5)\n",
        "    dZ4 = self.FC5.backward(dA5) \n",
        "    dA4 = self.activation4.backward(dZ4)\n",
        "    dZ3 = self.FC4.backward(dA4) \n",
        "    dA3 = self.activation3.backward(dZ3)\n",
        "    dZ2 = self.FC3.backward(dA3) \n",
        "    dA2 = self.activation2.backward(dZ2)\n",
        "    dZ1 = self.FC2.backward(dA2) \n",
        "    dA1 = self.activation1.backward(dZ1)\n",
        "    dZ0 = self.FC1.backward(dA1) \n",
        "\n",
        "  def predict(self, X):\n",
        "    self.forward(X)\n",
        "    return np.argmax(self.Z6, axis=1)\n"
      ],
      "metadata": {
        "id": "5jVnvbbsL2li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SDNN6 = ScratchDeepNeuralNetrowkClassifier_6(verbose=True, epoch=10, optimizer= AdaGrad, initializer = HeInitializer, activater = ReLU)\n",
        "\n",
        "SDNN6.fit(X_train, y_train_one_hot, X_val, y_test_one_hot) \n",
        "\n",
        "pred = SDNN6.predict(X_val)\n",
        "accuracy_score(y_val, pred)"
      ],
      "metadata": {
        "id": "j-2TcxwwTS-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(list(range(1, SDNN6.epoch+1)), SDNN6.loss_train, label='train')\n",
        "plt.plot(list(range(1, SDNN6.epoch+1)), SDNN6.loss_test, label='test')\n",
        "plt.legend()\n",
        "plt.xticks(list(range(1, SDNN6.epoch+1)));"
      ],
      "metadata": {
        "id": "DLpO-FRQTr9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, we can say that the accuracy was at its best when we had three layers. \n",
        "\n",
        "Let's finally generalize the ScratchDeepNeuralNetrwokClassifier and make it possible to input layers with the number of nodes. "
      ],
      "metadata": {
        "id": "zhdNIJaxUGKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class General_ScratchDeepNeuralNetrowkClassifier():\n",
        "  \n",
        "  def __init__(self, verbose=False, epoch=1, optimizer= SGD, initializer = HeInitializer, activater = ReLU, n_nodes=None):\n",
        "    self.verbose = verbose\n",
        "    self.batch_size = 20\n",
        "    self.sigma = 0.02\n",
        "    self.lr = 0.5\n",
        "    self.epoch = epoch\n",
        "    self.optimizer = optimizer\n",
        "    self.initializer = initializer\n",
        "    self.activater = activater\n",
        "    self.n_nodes = n_nodes\n",
        "\n",
        "  def fit(self, X, y, X_val=None, y_val=None):\n",
        "    self.loss_train = []\n",
        "    self.loss_val = []\n",
        "    optimizer = self.optimizer(self.lr)\n",
        "    self.fcs = []\n",
        "    self.act = []\n",
        "\n",
        "    for i in range(len(self.n_nodes)-2):\n",
        "      self.fcs.append(FC(self.n_nodes[i], self.n_nodes[i+1], self.initializer(self.sigma), optimizer))\n",
        "      self.act.append(self.activater())\n",
        "    self.fcs.append(FC(self.n_nodes[i], self.n_nodes[-1], self.initializer(self.sigma), optimizer))\n",
        "    self.act.append(softmax)\n",
        "\n",
        "    for i in range(self.epoch):\n",
        "      get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=i)\n",
        "      for mini_X, mini_y in get_mini_batch:\n",
        "        A = []\n",
        "        Z = []\n",
        "        for i, (f, a) in enumerate(zip(self.fcs, self.act)):\n",
        "          if i == 0:\n",
        "            A.append(f.forward(mini_X))  \n",
        "            Z.append(a.forward(A[i]))\n",
        "          else:\n",
        "            A.append(f.forward(Z[i-1]))   \n",
        "            Z.append(a.forward(A[i]))\n",
        "\n",
        "        dA = []\n",
        "        dZ = []\n",
        "        for i, (f, a) in enumerate(zip(self.fcs[::-1], self.act[::-1])):\n",
        "          if i == 0:\n",
        "            dA.append(a.backward(Z[-(i+1)], mini_y)[0])  \n",
        "            dZ.append(f.backward(dA[i]))\n",
        "          else:\n",
        "            dA.append(a.backward(dZ[i-1]))   \n",
        "            dZ.append(f.backward(dA[i])) \n",
        "\n",
        "        if self.verbose:\n",
        "          A = [] \n",
        "          Z = []\n",
        "          for i, (f, a) in enumerate(zip(self.fcs, self.act)):\n",
        "            if i == 0:\n",
        "              A.append(f.forward(X))\n",
        "              Z.append(a.forward(A[i]))\n",
        "            else:\n",
        "              A.append(f.forward(Z[i-1]))   \n",
        "              Z.append(a.forward(A[i]))\n",
        "          self.loss_train.append(self.act[-1].backward(Z[-1], y)[1])\n",
        "\n",
        "          if X_val is not None:\n",
        "            A = []\n",
        "            Z = []\n",
        "            for i, (f, a) in enumerate(zip(self.fcs, self.act)):\n",
        "            if i == 0:\n",
        "              A.append(f.forward(X_val))\n",
        "              Z.append(a.forward(A[i]))\n",
        "            else:\n",
        "              A.append(f.forward(Z[i-1]))   \n",
        "              Z.append(a.forward(A[i]))\n",
        "          self.loss_val.append(self.act[-1].backward(Z[-1], y_val)[1])\n",
        "\n",
        "  def predict(self, X):\n",
        "    A = []\n",
        "    Z = []\n",
        "    for i, (f, a) in enumerate(zip(self.fcs, self.act)):\n",
        "      if i == 0:\n",
        "        A.append(f.forward(X))\n",
        "        Z.append(a.forward(A[i]))\n",
        "      else:\n",
        "        A.append(f.forward(Z[i-1]))   \n",
        "        Z.append(a.forward(A[i]))\n",
        "    return np.argmax(Z[-1], axis=1)\n",
        "  \n",
        "    \n"
      ],
      "metadata": {
        "id": "QOXKxpH2VUcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it with five layers"
      ],
      "metadata": {
        "id": "et0z6NVGgrd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "node_list = [784, 400, 200, 150, 100, 10]\n",
        "SDNN5 = General_ScratchDeepNeuralNetrowkClassifier(verbose=True, epoch=10, optimizer= AdaGrad, initializer = HeInitializer, activater = ReLU, n_nodes=None)\n",
        "\n",
        "SDNN5.fit(X_train, y_train_one_hot, X_val, y_test_one_hot) \n",
        "\n",
        "pred = SDNN5.predict(X_val)\n",
        "accuracy_score(y_val, pred)"
      ],
      "metadata": {
        "id": "YegKsBGLgxN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(list(range(1, SDNN5.epoch+1)), SDNN5.loss_train, label='train')\n",
        "plt.plot(list(range(1, SDNN5.epoch+1)), SDNN5.loss_test, label='test')\n",
        "plt.legend()\n",
        "plt.xticks(list(range(1, SDNN5.epoch+1)));"
      ],
      "metadata": {
        "id": "otTDcKhDh8-e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}