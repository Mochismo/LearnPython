{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing_Object Detection and Tracking.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNExHogVfKEiiUqqGs+mObK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mochismo/LearnPython/blob/main/Preprocessing_Object_Detection_and_Tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkTKWIBz1HaZ",
        "outputId": "a49b8870-d625-46ce-917b-7c65fe75e915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 processed..\n",
            "100 processed..\n",
            "200 processed..\n",
            "300 processed..\n",
            "400 processed..\n",
            "500 processed..\n",
            "600 processed..\n",
            "700 processed..\n",
            "800 processed..\n",
            "0 processed..\n",
            "100 processed..\n",
            "200 processed..\n",
            "300 processed..\n",
            "400 processed..\n",
            "500 processed..\n",
            "600 processed..\n",
            "700 processed..\n",
            "800 processed..\n",
            "0 processed..\n",
            "100 processed..\n",
            "200 processed..\n",
            "300 processed..\n",
            "400 processed..\n",
            "500 processed..\n",
            "600 processed..\n",
            "700 processed..\n",
            "800 processed..\n",
            "0 processed..\n",
            "100 processed..\n",
            "200 processed..\n",
            "300 processed..\n",
            "400 processed..\n",
            "500 processed..\n",
            "600 processed..\n",
            "700 processed..\n",
            "800 processed..\n",
            "0 processed..\n",
            "100 processed..\n",
            "200 processed..\n",
            "300 processed..\n",
            "400 processed..\n",
            "500 processed..\n",
            "600 processed..\n",
            "700 processed..\n",
            "800 processed..\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Nov 30 14:46:20 2020\n",
        "\n",
        "@author: scholar1\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import math\n",
        "import cv2\n",
        "import pickle\n",
        "\n",
        "# scale image initially so that crop_size covers the target feature well\n",
        "scale_ratio = 0.25 # this should be 1/3?\n",
        "\n",
        "control_point_offset = np.array([[0.05, 0, 0], [-0.05, 0, 0], [0, 0.05, 0], [0, -0.05, 0], [0, 0, 0.05], [0, 0, -0.05]])\n",
        "\n",
        "def generate_output_directories(output_dir, num_data):\n",
        "\tif os.path.exists(output_dir) == False:\n",
        "\t\tos.mkdir(output_dir)\n",
        "\n",
        "\tfor i in range(num_data):\n",
        "\t\tpart_dir = os.path.join(output_dir, str(i))\n",
        "\t\tif os.path.exists(part_dir) == False:\n",
        "\t\t\tos.mkdir(part_dir)\n",
        "            \n",
        "    #marker points for backgound (refference or zero point)\n",
        "\tbackground_dir = os.path.join(output_dir, \"background\")\n",
        "\tif os.path.exists(background_dir) == False:\n",
        "\t\tos.mkdir(background_dir)\n",
        "\n",
        "\tfor i in range(num_data):\n",
        "\t\tpart_dir = os.path.join(output_dir, \"64x64_\" + str(i))\n",
        "\t\tif os.path.exists(part_dir) == False:\n",
        "\t\t\tos.mkdir(part_dir)\n",
        "\n",
        "\n",
        "def read_poses(path_to_data):\n",
        "\tfile_name = os.path.join(data_dir, \"poseGT.txt\")\n",
        "\n",
        "\tf = open(file_name, 'r')\n",
        "\n",
        "\tdict = {}\n",
        "\n",
        "\twith f as open_file_object:\n",
        "\t\tfor line in open_file_object:\n",
        "\t\t\telements = line.split()\n",
        "\t\t\tfloats = [float(x) for x in elements[1:]]\n",
        "\t\t\tdict[elements[0]] = floats\n",
        "\n",
        "\treturn dict\n",
        "\n",
        "def read_point_file(file_path):\n",
        "\tpoints = []\n",
        "\twith open(file_path) as f:\n",
        "\t\tcontent = f.readlines()\n",
        "\n",
        "\t\tfor line in content:\n",
        "\t\t\telements = line.split()\n",
        "\t\t\tpoints.append([float(x) for x in elements])\n",
        "        \n",
        "\treturn points\n",
        "       \n",
        "def clamp(x, min_val, max_val):\n",
        "\treturn min(max(x, min_val), max_val)\n",
        "\n",
        "def difference_of_Gaussians(img):\n",
        "\t#run a 5x5 gaussian blur then a 3x3 gaussian blr\n",
        "\tkernel1 = cv2.getGaussianKernel(10, 1)\n",
        "\tkernel2 = cv2.getGaussianKernel(10, 3)\n",
        "\n",
        "\tdog_img = cv2.sepFilter2D(img, -1, kernel1 - kernel2, kernel1 - kernel2)\n",
        "\tdog_img = cv2.normalize(dog_img, dog_img, alpha = 0, beta = 255, norm_type=cv2.NORM_MINMAX)\n",
        "\n",
        "\treturn dog_img\n",
        "\n",
        "\n",
        "def output_cropped_image(x, y, img, output_path, crop_size, apply_dog = False):\n",
        "\thalf_crop_size = int(crop_size / 2)\n",
        "\tx_min = x - half_crop_size\n",
        "\tx_max = x + half_crop_size\n",
        "\ty_min = y - half_crop_size\n",
        "\ty_max = y + half_crop_size\n",
        "\tif x_min < 0 or x_max >= np.size(img,1) or y_min < 0 or y_max >= np.size(img,0):\n",
        "\t\treturn False\n",
        "\n",
        "\tcrop = img[y_min:y_max, x_min:x_max]\n",
        "\tif apply_dog:\n",
        "\t\tcrop = difference_of_Gaussians(crop)\n",
        "\n",
        "\tif crop.shape != (crop_size, crop_size):\n",
        "\t\tprint(\"crop.shape\", crop.shape, \" something is wrong..\")\n",
        "\t\tprint(x_min, x_max, y_min, y_max)\n",
        "\t\tsys.exit(1)\n",
        "\n",
        "\tcv2.imwrite(output_path, crop)\n",
        "\n",
        "\treturn True\n",
        "\n",
        "def write_part_detection_training_img(img, i, annotation_projected_points, data_id, output_dir):\n",
        "\tcrop_size = 32\n",
        "\n",
        "\tfor j in range(len(annotation_projected_points)):\n",
        "\t\tprojected_point = (annotation_projected_points[j] * scale_ratio).astype(int)\n",
        "\t\t#print(annotation_projected_points[j], projected_point)\n",
        "\t\toutput_path = os.path.join(output_dir, str(j), str(data_id) + \"_\" + str(i).zfill(5) + \".png\")\n",
        "\t\toutput_cropped_image(projected_point[0][0], projected_point[0][1], img, output_path, crop_size, True)\n",
        "\n",
        "\t# generate background example\n",
        "\twhile True:\n",
        "\t\thalf_crop_size = int(crop_size / 2)\n",
        "\t\tx = np.random.randint(half_crop_size, np.size(img,1)-half_crop_size)\n",
        "\t\ty = np.random.randint(half_crop_size, np.size(img,0)-half_crop_size)\n",
        "\t\tclose_to_annotated_points = False\n",
        "\t\tfor j in range(len(annotation_projected_points)):\n",
        "\t\t\tprojected_point = (annotation_projected_points[j] * scale_ratio).astype(int)\n",
        "\t\t\tif abs(x - projected_point[0][0]) <= half_crop_size or abs(y - projected_point[0][1]) <= half_crop_size:\n",
        "\t\t\t\tclose_to_annotated_points = True\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\tif close_to_annotated_points == False:\n",
        "\t\t\t# region not containing any annotated part is found\n",
        "\t\t\toutput_path = os.path.join(output_dir, \"background\", str(data_id) + \"_\" + str(i).zfill(5) + \".png\")\n",
        "\t\t\toutput_cropped_image(x, y, img, output_path, crop_size, True)\n",
        "\t\t\tbreak\n",
        "\n",
        "def write_control_points_detection_training_img(img, i, annotation_projected_points, data_id, output_dir):\n",
        "\tcrop_size = 64\n",
        "\tis_cropped_image_saved = []\n",
        "\n",
        "\tfor j in range(len(annotation_projected_points)):\n",
        "\t\tprojected_point = (annotation_projected_points[j] * scale_ratio).astype(int)\n",
        "\t\t#print(annotation_projected_points[j], projected_point)\n",
        "\t\toutput_path = os.path.join(output_dir, \"64x64_\" + str(j), str(data_id) + \"_\" + str(i).zfill(5) + \".png\")\n",
        "\t\tsuccess = output_cropped_image(projected_point[0][0], projected_point[0][1], img, output_path, crop_size)\n",
        "\t\tis_cropped_image_saved.append(success)\n",
        "\n",
        "\treturn is_cropped_image_saved\n",
        "\n",
        "\n",
        "def extract_images(data_id, data_dir, output_dir, all_projected_control_points):\n",
        "\tvisualization = False\n",
        "\n",
        "\n",
        "    \n",
        "\tpath_to_data = os.path.join(data_dir, \"frame0000\" + str(data_id))\n",
        "    #\n",
        "\n",
        "\tif visualization == True:\n",
        "\t\tmarker_3d_points = np.array(read_point_file(os.path.join(path_to_data, \"markers3dPoints.txt\")))\n",
        "\t\tmarker_3d_points.reshape(-1, 3)\n",
        "\n",
        "\tcontrol_points_for_all_parts = []\n",
        "\tfor annotation_pt in annotation_3d_points:\n",
        "\t\tcontrol_pts_for_part = []\n",
        "\t\tfor offset in control_point_offset:\n",
        "\t\t\tcontrol_pts_for_part.append(annotation_pt + offset)\n",
        "\t\t\n",
        "\t\tcontrol_points_for_all_parts.append(control_pts_for_part)\n",
        "\n",
        "\tcontrol_points_for_all_parts = np.array(control_points_for_all_parts)\n",
        "\t#assert control_points_for_all_parts.shape == (4, 6, 3)\n",
        "\n",
        "   \n",
        "\n",
        "\tRts = read_poses(path_to_data)\n",
        "\tcamera_matrix = np.float32([[2666.67, 0, 960], [0, 2666.67, 540], [0, 0, 1.]])\n",
        "    \n",
        "\n",
        "\t#index = 0\n",
        "\n",
        "\tfor i in range(0, len(Rts)):\n",
        "\t\tfile_name = \"frame\" + str(i).zfill(5) + \".png\"\n",
        "\t\timg_path = os.path.join(data_dir, file_name)\n",
        "\t\timg = cv2.imread(img_path)\n",
        "\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\t\timg = cv2.resize(img, None, fx=scale_ratio, fy=scale_ratio, interpolation=cv2.INTER_AREA) \n",
        "        \n",
        "\n",
        "\t\tRt = Rts[file_name]\n",
        "\n",
        "\t\tif math.isnan(Rt[0]) == True:\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\tannotation_projected_points, jac = cv2.projectPoints(annotation_3d_points, np.array(Rt[0:3]), np.array(Rt[3:6]), camera_matrix, np.float32([[0, 0, 0, 0, 0]]))\n",
        "#\t\tassert annotation_projected_points.shape == (4, 1, 2)\n",
        "        \n",
        "\t\twrite_part_detection_training_img(img, i, annotation_projected_points, data_id, output_dir)\n",
        "\n",
        "\t\tis_cropped_image_saved = write_control_points_detection_training_img(img, i, annotation_projected_points, data_id, output_dir)\n",
        "\n",
        "\t\tfor j in range(control_points_for_all_parts.shape[0]):\n",
        "\t\t\tif is_cropped_image_saved[j] == False:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\tprojected_control_points_for_part, jac = cv2.projectPoints(control_points_for_all_parts[j], np.array(Rt[0:3]), np.array(Rt[3:6]), camera_matrix, np.float32([[0, 0, 0, 0, 0]]))\n",
        "           # assert projected_control_points_for_part.shape == (6, 1, 2)\n",
        "\n",
        "\t\t\t# set relative offset from annotation_projected_points[j] here\n",
        "\t\t\tall_projected_control_points[j].append(projected_control_points_for_part - annotation_projected_points[j])\n",
        "\n",
        "\t\t# debug\n",
        "\t\tif visualization:\n",
        "\t\t\tcolor_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "\t\t\tfor part_id, projected_control_points_for_part in enumerate(all_projected_control_points):\n",
        "\t\t\t\tfor idx, point in enumerate(projected_control_points_for_part[-1]):\n",
        "\t\t\t\t\tcolor_code = idx % 6\n",
        "\t\t\t\t\tcolor = (0, 0, 255)\n",
        "\t\t\t\t\tif color_code == 2 or color_code == 3:\n",
        "\t\t\t\t\t\tcolor = (0, 255, 0)\n",
        "\t\t\t\t\tif color_code == 4 or color_code == 5:\n",
        "\t\t\t\t\t\tcolor = (255, 0, 0)\n",
        "\n",
        "\t\t\t\t\tpoint = (annotation_projected_points[part_id] + point) * scale_ratio\n",
        "\t\t\t\t\tcv2.circle(color_img, (int(point[0][0]), int(point[0][1])), 5, color, 1)\n",
        "\n",
        "\t\t\t\tcv2.putText(color_img, str(part_id), (int(point[0][0]), int(point[0][1])),\n",
        "\t\t\t\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "\t\t\tcv2.imwrite(os.path.join(\"debug\", str(i) + \".png\"), color_img)\n",
        "\n",
        "\t\tif i%100 == 0:\n",
        "\t\t\tprint(i, \"processed..\", flush=True)\n",
        "\n",
        "\t\tif False:\n",
        "\t\t\tfor point in annotation_projected_points:\n",
        "\t\t\t\tprint(\"point.shape\", point.shape)\n",
        "\t\t\t\tcv2.circle(img, (int(point[0][0]), int(point[0][1])), 10, (0, 255, 0), 2)\n",
        "\n",
        "\t\t\tmarker_projected_points, jac = cv2.projectPoints(marker_3d_points, np.array(Rt[0:3]), np.array(Rt[3:6]), camera_matrix, np.empty(0))\n",
        "\t\t\tfor point in marker_projected_points:\n",
        "\t\t\t\tprint(\"point.shape\", point.shape)\n",
        "\t\t\t\tcv2.circle(img, (int(point[0][0]), int(point[0][1])), 10, (0, 100, 255), 2)\n",
        "\n",
        "\t\t\tcv2.imwrite('out.png',img)\n",
        "\t\t\tcv2.imshow('image',img)\n",
        "\t\t\tk = cv2.waitKey(0)\n",
        "\t\t\tif k == 27:         # wait for ESC key to exit\n",
        "\t\t\t    cv2.destroyAllWindows()\n",
        "\t\t\t    sys.exit(0)\n",
        "\t\t\t# elif k == ord('s'): # wait for 's' key to save and exit\n",
        "\t\t\t#     cv2.imwrite('messigray.png',img)\n",
        "\t\t\t#     cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "\n",
        "#if len(sys.argv) != 2:\n",
        "#\tprint(\"usage: \" + sys.argv[0] + \" data_dir\")\n",
        "#\tsys.exit(1)\n",
        "\n",
        "#data_dir = sys.argv[1]\n",
        "data_dir = \"/content/drive/MyDrive/video1/\"\n",
        "\n",
        "annotation_3d_points = np.array(read_point_file(os.path.join(data_dir, \"markers3dPoints.txt\")))\n",
        "\n",
        "#annotation_3d_points = np.array(read_point_file(os.path.join(data_dir, \"C:\\Users\\scholar1\\Desktop\\swaymotion\\video1\\markers3dPoints.txt\")))\n",
        "annotation_3d_points.reshape(-1, 3)\n",
        "\n",
        "# extract only 4 representative parts\n",
        "annotation_3d_points = np.delete(annotation_3d_points, [1, 3, 5, 7], 0)\n",
        "assert annotation_3d_points.shape == (16, 3)\n",
        "\n",
        "output_dir = os.path.join(data_dir, \"training\")\n",
        "generate_output_directories(output_dir, len(annotation_3d_points))\n",
        "\n",
        "\n",
        "all_projected_control_points = []\n",
        "for i in range(len(annotation_3d_points)):\n",
        "\tall_projected_control_points.append([])\n",
        "\n",
        "for data_id in range(1,6):\n",
        "\textract_images(data_id, data_dir, output_dir, all_projected_control_points)\n",
        "\n",
        "\n",
        "for part_id, projected_control_points_for_part in enumerate(all_projected_control_points):\n",
        "\tfile_name = \"projected_control_points_\" + str(part_id) + \".data\"\n",
        "\twith open(os.path.join(data_dir, \"training\", file_name), 'wb') as filehandle:\n",
        "\t\t# store the data as binary data stream\n",
        "\n",
        "\t\tpickle.dump(projected_control_points_for_part, filehandle)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Nov 30 11:39:37 2020\n",
        "\n",
        "@author: scholar1\n",
        "\"\"\"\n",
        "#from pyimagesearch.minivggnet import MiniVGGNet\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from imutils import build_montages\n",
        "from tensorflow.keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "import sys\n",
        "\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)\n",
        "# initialize the number of epochs to train for, base learning rate,\n",
        "# and batch size\n",
        "NUM_EPOCHS = 20\n",
        "INIT_LR = 1e-2\n",
        "BS = 32\n",
        "\n",
        "def build_model(width, height, depth, classes):\n",
        "\t# initialize the model along with the input shape to be\n",
        "\t# \"channels last\" and the channels dimension itself\n",
        "\tmodel = tf.keras.models.Sequential()\n",
        "\tinputShape = (height, width, depth)\n",
        "\tchanDim = -1\n",
        "\n",
        "\t# if we are using \"channels first\", update the input shape\n",
        "\t# and channels dimension\n",
        "\tif K.image_data_format() == \"channels_first\":\n",
        "\t\tinputShape = (depth, height, width)\n",
        "\t\tchanDim = 1\n",
        "\n",
        "\t# first CONV => RELU => CONV => RELU => POOL layer set\n",
        "\tmodel.add(tf.keras.layers.Conv2D(20, (5, 5), padding=\"valid\", activation=\"relu\", input_shape=inputShape))\n",
        "\tmodel.add(tf.keras.layers.BatchNormalization(axis=chanDim))\n",
        "\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\tmodel.add(tf.keras.layers.Conv2D(50, (5, 5), padding=\"valid\", activation=\"relu\"))\n",
        "\tmodel.add(tf.keras.layers.BatchNormalization(axis=chanDim))\n",
        "\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\t# first (and only) set of FC => RELU layers\n",
        "\tmodel.add(tf.keras.layers.Flatten())\n",
        "\tmodel.add(tf.keras.layers.Dense(500, activation=\"relu\"))\n",
        "\tmodel.add(tf.keras.layers.BatchNormalization())\n",
        "\t\n",
        "\t# softmax classifier\n",
        "\tmodel.add(tf.keras.layers.Dense(classes, activation=\"softmax\"))\n",
        "\n",
        "\t# return the constructed network architecture\n",
        "\treturn model\n",
        "\n",
        "\n",
        "def get_all_images_for_label(label, X, Y):\t\n",
        "\timg_paths = glob.glob(os.path.join(\"/content/drive/MyDrive/video1/training\", label, \"*.png\"))\n",
        "\tfor img_path in img_paths:\n",
        "\t\tX.append(cv2.imread(img_path, cv2.IMREAD_GRAYSCALE))\n",
        "\tif label != \"background\":\n",
        "\t\tY.extend(int(label) for i in range(len(img_paths)))\n",
        "\n",
        "\telse:\n",
        "\t\tY.extend(4 for i in range(len(img_paths)))\n",
        "#data_dir = \"video1/\"\n",
        "#annotation_3d_points = np.array(read_point_file(os.path.join(data_dir, \"markers3dPoints.txt\")))\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "get_all_images_for_label(\"0\", X, Y)\n",
        "get_all_images_for_label(\"1\", X, Y)\n",
        "get_all_images_for_label(\"2\", X, Y)\n",
        "get_all_images_for_label(\"3\", X, Y)\n",
        "get_all_images_for_label(\"background\", X, Y)\n",
        "\n",
        "# convert from python list to numpy array\n",
        "X = np.array(X)\n",
        "Y = np.array(Y) \n",
        "\n",
        "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.1, shuffle= True)\n",
        "\n",
        "print(\"trainX.shape\", trainX.shape)\n",
        "print(\"trainY.shape\", trainY.shape)\n",
        "print(\"testX.shape\", testX.shape) \n",
        "print(\"testY.shape\", testY.shape) \n",
        "\n",
        "img_width = trainX[0].shape[1]\n",
        "img_height = trainX[0].shape[0]\n",
        "\n",
        "\n",
        "# if we are using \"channels first\" ordering, then reshape the design\n",
        "# matrix such that the matrix is:\n",
        "# \tnum_samples x depth x rows x columns\n",
        "if K.image_data_format() == \"channels_first\":\n",
        "\ttrainX = trainX.reshape((trainX.shape[0], 1, img_width, img_height))\n",
        "\ttestX = testX.reshape((testX.shape[0], 1, img_width, img_height))\n",
        " \n",
        "# otherwise, we are using \"channels last\" ordering, so the design\n",
        "# matrix shape should be: num_samples x rows x columns x depth\n",
        "else:\n",
        "\ttrainX = trainX.reshape((trainX.shape[0], img_width, img_height, 1))\n",
        "\ttestX = testX.reshape((testX.shape[0], img_width, img_height, 1))\n",
        " \n",
        "# scale data to the range of [0, 1]\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "\n",
        "num_classes = 5\n",
        "\n",
        "# one-hot encode the training and testing labels\n",
        "trainY = tf.keras.utils.to_categorical(trainY, num_classes)\n",
        "testY = tf.keras.utils.to_categorical(testY, num_classes)\n",
        "\n",
        "labelNames = [\"0 \", \"1\", \"2\", \"3\", \"background\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# initialize the optimizer and model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = tf.keras.optimizers.SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR / NUM_EPOCHS)\n",
        "model = build_model(width=img_width, height=img_height, depth=1, classes=num_classes)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# train the network\n",
        "print(\"[INFO] training model...\")\n",
        "H = model.fit(trainX, trainY,\n",
        "\tvalidation_data=(testX, testY),\n",
        "\tbatch_size=BS, epochs=NUM_EPOCHS)\n",
        "\n",
        "model.save('model_part_detection.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "\n",
        "hist = H.history\n",
        "x_arr = np.arange(len(hist['loss'])) + 1\n",
        "fig = plt.figure(figsize=(12,4))\n",
        "ax = fig.add_subplot(1, 3, 1)\n",
        "ax.plot(x_arr, hist['loss'], '-o', label='Train loss')\n",
        "ax.plot(x_arr, hist['val_loss'], '--<', label='Validation loss')\n",
        "ax.legend (fontsize=15)\n",
        "ax = fig.add_subplot(1, 3, 2)\n",
        "ax.plot(x_arr, hist['accuracy'], '-o', label='Train acc.')\n",
        "ax.plot(x_arr, hist['val_accuracy'], '--<',\n",
        "        label='Validation acc.')\n",
        "#ax.legend(fontsize=15)\n",
        "#ax = fig.add_subplot(1, 3, 3)\n",
        "#plot_decision_regions(X= testX, clf=model)\n",
        "#ax.set_xlabel(r'$x_1$', size=15)\n",
        "#ax.xaxis.set_label.coords(1, -0.025)\n",
        "#ax.set_ylabel(r'$x_2$', size=15)\n",
        "#ax.yaxis.set_label.coords(-0.025, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eSL2fLPMREnR",
        "outputId": "464d1a4b-eefa-47dd-f977-19f11323194f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainX.shape (17172, 32, 32)\n",
            "trainY.shape (17172,)\n",
            "testX.shape (1908, 32, 32)\n",
            "testY.shape (1908,)\n",
            "[INFO] compiling model...\n",
            "Model: \"sequential\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 20)        520       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 28, 28, 20)       80        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 20)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 10, 10, 50)        25050     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 10, 10, 50)       200       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 50)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1250)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 500)               625500    \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 500)              2000      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 2505      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 655,855\n",
            "Trainable params: 654,715\n",
            "Non-trainable params: 1,140\n",
            "_________________________________________________________________\n",
            "None\n",
            "[INFO] training model...\n",
            "Epoch 1/20\n",
            "537/537 [==============================] - 27s 48ms/step - loss: 0.0441 - accuracy: 0.9860 - val_loss: 0.0069 - val_accuracy: 0.9995\n",
            "Epoch 2/20\n",
            "537/537 [==============================] - 26s 48ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 3/20\n",
            "537/537 [==============================] - 25s 47ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 4.2433e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/20\n",
            "537/537 [==============================] - 25s 47ms/step - loss: 6.8134e-04 - accuracy: 0.9999 - val_loss: 1.0790e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/20\n",
            "537/537 [==============================] - 26s 48ms/step - loss: 6.1089e-04 - accuracy: 0.9999 - val_loss: 8.2428e-05 - val_accuracy: 1.0000\n",
            "Epoch 6/20\n",
            "537/537 [==============================] - 25s 47ms/step - loss: 4.7017e-04 - accuracy: 0.9999 - val_loss: 1.0583e-04 - val_accuracy: 1.0000\n",
            "Epoch 7/20\n",
            "537/537 [==============================] - 26s 49ms/step - loss: 2.3590e-04 - accuracy: 1.0000 - val_loss: 6.4628e-05 - val_accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "537/537 [==============================] - 27s 50ms/step - loss: 2.1023e-04 - accuracy: 1.0000 - val_loss: 8.6943e-05 - val_accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "537/537 [==============================] - 26s 49ms/step - loss: 2.8288e-04 - accuracy: 1.0000 - val_loss: 7.1463e-05 - val_accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "537/537 [==============================] - 26s 49ms/step - loss: 2.3645e-04 - accuracy: 1.0000 - val_loss: 5.2311e-05 - val_accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "537/537 [==============================] - 27s 50ms/step - loss: 2.1936e-04 - accuracy: 0.9999 - val_loss: 4.5825e-05 - val_accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "537/537 [==============================] - 27s 50ms/step - loss: 1.5611e-04 - accuracy: 1.0000 - val_loss: 4.1517e-05 - val_accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "537/537 [==============================] - 26s 49ms/step - loss: 2.3727e-04 - accuracy: 1.0000 - val_loss: 3.4853e-05 - val_accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "537/537 [==============================] - 27s 51ms/step - loss: 1.3220e-04 - accuracy: 1.0000 - val_loss: 4.5671e-05 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "537/537 [==============================] - 27s 50ms/step - loss: 2.9158e-04 - accuracy: 0.9999 - val_loss: 3.7265e-05 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "537/537 [==============================] - 28s 52ms/step - loss: 1.4712e-04 - accuracy: 1.0000 - val_loss: 3.9802e-05 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "537/537 [==============================] - 26s 49ms/step - loss: 1.7934e-04 - accuracy: 1.0000 - val_loss: 3.8583e-05 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "537/537 [==============================] - 27s 49ms/step - loss: 1.6381e-04 - accuracy: 1.0000 - val_loss: 3.6735e-05 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "537/537 [==============================] - 26s 49ms/step - loss: 1.6304e-04 - accuracy: 1.0000 - val_loss: 3.7300e-05 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "537/537 [==============================] - 27s 50ms/step - loss: 3.9540e-04 - accuracy: 0.9999 - val_loss: 4.2790e-05 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Nov 30 12:07:05 2020\n",
        "\n",
        "@author: scholar1\n",
        "\"\"\"\n",
        "#from pyimagesearch.minivggnet import MiniVGGNet\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from imutils import build_montages\n",
        "from tensorflow.keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "import pickle\n",
        "\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)\n",
        "# initialize the number of epochs to train for, base learning rate,\n",
        "# and batch size\n",
        "NUM_EPOCHS = 25\n",
        "INIT_LR = 1e-2\n",
        "BS = 32\n",
        "\n",
        "def build_model(width, height, depth, classes):\n",
        "\t# initialize the model along with the input shape to be\n",
        "\t# \"channels last\" and the channels dimension itself\n",
        "\tmodel = tf.keras.models.Sequential()\n",
        "\tinputShape = (height, width, depth)\n",
        "\tchanDim = -1\n",
        "\n",
        "\t# if we are using \"channels first\", update the input shape\n",
        "\t# and channels dimension\n",
        "\tif K.image_data_format() == \"channels_first\":\n",
        "\t\tinputShape = (depth, height, width)\n",
        "\t\tchanDim = 1\n",
        "\n",
        "\t# first CONV => RELU => CONV => RELU => POOL layer set\n",
        "\tmodel.add(tf.keras.layers.Conv2D(20, (5, 5), padding=\"valid\", activation=\"relu\", input_shape=inputShape))\n",
        "\tmodel.add(tf.keras.layers.BatchNormalization(axis=chanDim))\n",
        "\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\tmodel.add(tf.keras.layers.Conv2D(50, (5, 5), padding=\"valid\", activation=\"relu\"))\n",
        "\tmodel.add(tf.keras.layers.BatchNormalization(axis=chanDim))\n",
        "\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\t# first (and only) set of FC => RELU layers\n",
        "\tmodel.add(tf.keras.layers.Flatten())\n",
        "\tmodel.add(tf.keras.layers.Dense(1024, activation=\"relu\"))\n",
        "\tmodel.add(tf.keras.layers.BatchNormalization())\n",
        "\t\n",
        "\t# softmax classifier\n",
        "\tmodel.add(tf.keras.layers.Dense(classes, activation=\"linear\"))\n",
        "\n",
        "\t# return the constructed network architecture\n",
        "\treturn model\n",
        "\n",
        "\n",
        "def get_all_images_for_label(label, X, Y):\t\n",
        "\timg_paths = glob.glob(os.path.join(\"training\", \"64x64_\" + label, \"*.png\"))\n",
        "\tfor img_path in img_paths:\n",
        "\t\tX.append(cv2.imread(img_path, cv2.IMREAD_GRAYSCALE))\n",
        "\t#print(\"c\")\n",
        "\tif label != \"background\":\n",
        "\t\tY.extend(int(label) for i in range(len(img_paths)))\n",
        "\telse:\n",
        "\t\tY.extend(4 for i in range(len(img_paths)))\n",
        "\n",
        "num_parts = 4\n",
        "\n",
        "for i in range(num_parts):\n",
        "\tX = []\n",
        "\tY = []\n",
        "\n",
        "\timg_paths = sorted(glob.glob(os.path.join(\"/content/drive/MyDrive/video1/training\", \"64x64_\" + str(i), \"*.png\")))\n",
        "\tfor img_path in img_paths:\n",
        "\t\tX.append(cv2.imread(img_path, cv2.IMREAD_GRAYSCALE))\n",
        "\n",
        "\tfile_name = \"/content/drive/MyDrive/video1/training/projected_control_points_\" + str(i) + \".data\"\n",
        "\twith open(file_name, 'rb') as filehandle:  \n",
        "\t\tY = pickle.load(filehandle)\n",
        "\n",
        "\t# convert from python list to numpy array\n",
        "\tX = np.array(X)\n",
        "\tY = np.array(Y) \n",
        "\n",
        "\tnum_control_points = Y.shape[1]\n",
        "\t# reshape control points in 1d vector\n",
        "\tY = Y.reshape((Y.shape[0], num_control_points * 2))\n",
        "\n",
        "\tprint(X.shape, Y.shape)\n",
        "\ttrainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.1, shuffle= True)\n",
        "\n",
        "\tprint(\"trainX.shape\", trainX.shape)\n",
        "\tprint(\"trainY.shape\", trainY.shape)\n",
        "\tprint(\"testX.shape\", testX.shape) \n",
        "\tprint(\"testY.shape\", testY.shape) \n",
        "\n",
        "\timg_width = trainX[0].shape[1]\n",
        "\timg_height = trainX[0].shape[0]\n",
        "\n",
        "\t# if we are using \"channels first\" ordering, then reshape the design\n",
        "\t# matrix such that the matrix is:\n",
        "\t# \tnum_samples x depth x rows x columns\n",
        "\tif K.image_data_format() == \"channels_first\":\n",
        "\t\ttrainX = trainX.reshape((trainX.shape[0], 1, img_width, img_height))\n",
        "\t\ttestX = testX.reshape((testX.shape[0], 1, img_width, img_height))\n",
        "\t\n",
        "\t# otherwise, we are using \"channels last\" ordering, so the design\n",
        "\t# matrix shape should be: num_samples x rows x columns x depth\n",
        "\telse:\n",
        "\t\ttrainX = trainX.reshape((trainX.shape[0], img_width, img_height, 1))\n",
        "\t\ttestX = testX.reshape((testX.shape[0], img_width, img_height, 1))\n",
        "\t\n",
        "\t# scale data to the range of [0, 1]\n",
        "\ttrainX = trainX.astype(\"float32\") / 255.0\n",
        "\ttestX = testX.astype(\"float32\") / 255.0\n",
        "\n",
        "\t# initialize the optimizer and model\n",
        "\tprint(\"[INFO] compiling model...\")\n",
        "\topt = tf.keras.optimizers.SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR / NUM_EPOCHS)\n",
        "\tmodel = build_model(width=img_width, height=img_height, depth=1, classes=num_control_points * 2)\n",
        "\tmodel.compile(loss=\"mean_squared_error\", optimizer=opt,\n",
        "\t\tmetrics=[\"accuracy\"])\n",
        "\n",
        "\tprint(model.summary())\n",
        "\n",
        "\t# train the network\n",
        "\tprint(\"[INFO] training model...\")\n",
        "\tH = model.fit(trainX, trainY,\n",
        "\t\tvalidation_data=(testX, testY),\n",
        "\t\tbatch_size=BS, epochs=NUM_EPOCHS)\n",
        "\n",
        "\tmodel.save(\"model_control_points_\" + str(i) + \".h5\")  # creates a HDF5 file 'my_model.h5'\n",
        "hist = H.history\n",
        "x_arr = np.arange(len(hist['loss'])) + 1\n",
        "fig = plt.figure(figsize=(12,4))\n",
        "ax = fig.add_subplot(1, 2, 1)\n",
        "ax.plot(x_arr, hist['loss'], '-o', label='Train loss')\n",
        "ax.plot(x_arr, hist['val_loss'], '--<', label='Validation loss')\n",
        "ax.legend (fontsize=15)\n",
        "ax = fig.add_subplot(1, 2, 2)\n",
        "ax.plot(x_arr, hist['accuracy'], '-o', label='Train acc.')\n",
        "ax.plot(x_arr, hist['val_accuracy'], '--<',\n",
        "        label='Validation acc.')\n",
        "ax.legend(fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CFa8HmfbUrkb",
        "outputId": "151ffd33-f036-4e26-ef81-182049a0b70c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1768: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1715, 64, 64) (1715, 12)\n",
            "trainX.shape (1543, 64, 64)\n",
            "trainY.shape (1543, 12)\n",
            "testX.shape (172, 64, 64)\n",
            "testY.shape (172, 12)\n",
            "[INFO] compiling model...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 60, 60, 20)        520       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 60, 60, 20)       80        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 30, 30, 20)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 26, 26, 50)        25050     \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 26, 26, 50)       200       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 13, 13, 50)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 8450)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1024)              8653824   \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 12)                12300     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,696,070\n",
            "Trainable params: 8,693,882\n",
            "Non-trainable params: 2,188\n",
            "_________________________________________________________________\n",
            "None\n",
            "[INFO] training model...\n",
            "Epoch 1/25\n",
            "49/49 [==============================] - 14s 272ms/step - loss: 435.4165 - accuracy: 0.8620 - val_loss: 1420.6805 - val_accuracy: 0.9593\n",
            "Epoch 2/25\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 8.0311 - accuracy: 0.9961 - val_loss: 243.6790 - val_accuracy: 0.9709\n",
            "Epoch 3/25\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 4.9422 - accuracy: 0.9955 - val_loss: 14.6985 - val_accuracy: 1.0000\n",
            "Epoch 4/25\n",
            "49/49 [==============================] - 14s 276ms/step - loss: 3.1159 - accuracy: 0.9981 - val_loss: 4.3904 - val_accuracy: 1.0000\n",
            "Epoch 5/25\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 3.0373 - accuracy: 0.9948 - val_loss: 3.9431 - val_accuracy: 1.0000\n",
            "Epoch 6/25\n",
            "49/49 [==============================] - 13s 265ms/step - loss: 3.5493 - accuracy: 0.9968 - val_loss: 1.3473 - val_accuracy: 1.0000\n",
            "Epoch 7/25\n",
            "49/49 [==============================] - 13s 264ms/step - loss: 3.4263 - accuracy: 0.9974 - val_loss: 7.6812 - val_accuracy: 1.0000\n",
            "Epoch 8/25\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 3.6596 - accuracy: 0.9981 - val_loss: 1.5818 - val_accuracy: 1.0000\n",
            "Epoch 9/25\n",
            "49/49 [==============================] - 13s 276ms/step - loss: 2.8100 - accuracy: 0.9974 - val_loss: 3.2900 - val_accuracy: 1.0000\n",
            "Epoch 10/25\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 3.1538 - accuracy: 0.9994 - val_loss: 1.4517 - val_accuracy: 1.0000\n",
            "Epoch 11/25\n",
            "49/49 [==============================] - 13s 265ms/step - loss: 1.7293 - accuracy: 1.0000 - val_loss: 0.8912 - val_accuracy: 1.0000\n",
            "Epoch 12/25\n",
            "49/49 [==============================] - 13s 265ms/step - loss: 2.7263 - accuracy: 0.9974 - val_loss: 0.8088 - val_accuracy: 1.0000\n",
            "Epoch 13/25\n",
            "49/49 [==============================] - 13s 265ms/step - loss: 2.3365 - accuracy: 0.9987 - val_loss: 1.1957 - val_accuracy: 1.0000\n",
            "Epoch 14/25\n",
            "49/49 [==============================] - 13s 265ms/step - loss: 2.5029 - accuracy: 0.9981 - val_loss: 0.8152 - val_accuracy: 1.0000\n",
            "Epoch 15/25\n",
            "49/49 [==============================] - 13s 265ms/step - loss: 2.2464 - accuracy: 0.9968 - val_loss: 1.0114 - val_accuracy: 1.0000\n",
            "Epoch 16/25\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 2.0697 - accuracy: 1.0000 - val_loss: 1.3263 - val_accuracy: 1.0000\n",
            "Epoch 17/25\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 1.8071 - accuracy: 0.9994 - val_loss: 0.9042 - val_accuracy: 1.0000\n",
            "Epoch 18/25\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 1.8956 - accuracy: 0.9981 - val_loss: 0.7786 - val_accuracy: 1.0000\n",
            "Epoch 19/25\n",
            "49/49 [==============================] - 13s 266ms/step - loss: 2.3167 - accuracy: 0.9987 - val_loss: 0.4011 - val_accuracy: 1.0000\n",
            "Epoch 20/25\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 2.6415 - accuracy: 0.9961 - val_loss: 3.2892 - val_accuracy: 1.0000\n",
            "Epoch 21/25\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 2.7838 - accuracy: 0.9955 - val_loss: 0.4086 - val_accuracy: 1.0000\n",
            "Epoch 22/25\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 1.8708 - accuracy: 0.9968 - val_loss: 0.3073 - val_accuracy: 1.0000\n",
            "Epoch 23/25\n",
            "49/49 [==============================] - 13s 267ms/step - loss: 1.5020 - accuracy: 1.0000 - val_loss: 1.3523 - val_accuracy: 1.0000\n",
            "Epoch 24/25\n",
            "49/49 [==============================] - 13s 269ms/step - loss: 2.7607 - accuracy: 0.9961 - val_loss: 0.9462 - val_accuracy: 1.0000\n",
            "Epoch 25/25\n",
            "49/49 [==============================] - 13s 268ms/step - loss: 2.1848 - accuracy: 0.9987 - val_loss: 0.3277 - val_accuracy: 1.0000\n",
            "(3890, 64, 64) (3890, 12)\n",
            "trainX.shape (3501, 64, 64)\n",
            "trainY.shape (3501, 12)\n",
            "testX.shape (389, 64, 64)\n",
            "testY.shape (389, 12)\n",
            "[INFO] compiling model...\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 60, 60, 20)        520       \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 60, 60, 20)       80        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 30, 30, 20)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 26, 26, 50)        25050     \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 26, 26, 50)       200       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 13, 13, 50)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 8450)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1024)              8653824   \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 12)                12300     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,696,070\n",
            "Trainable params: 8,693,882\n",
            "Non-trainable params: 2,188\n",
            "_________________________________________________________________\n",
            "None\n",
            "[INFO] training model...\n",
            "Epoch 1/25\n",
            "110/110 [==============================] - 30s 266ms/step - loss: 217.0999 - accuracy: 0.8995 - val_loss: 4365.3789 - val_accuracy: 0.0720\n",
            "Epoch 2/25\n",
            "110/110 [==============================] - 29s 264ms/step - loss: 5.4275 - accuracy: 0.9549 - val_loss: 55.5659 - val_accuracy: 0.9589\n",
            "Epoch 3/25\n",
            "110/110 [==============================] - 29s 265ms/step - loss: 4.2586 - accuracy: 0.9503 - val_loss: 6.4105 - val_accuracy: 0.9306\n",
            "Epoch 4/25\n",
            "110/110 [==============================] - 29s 266ms/step - loss: 4.5588 - accuracy: 0.9572 - val_loss: 4.8898 - val_accuracy: 0.9794\n",
            "Epoch 5/25\n",
            "110/110 [==============================] - 29s 266ms/step - loss: 4.4501 - accuracy: 0.9580 - val_loss: 10.3205 - val_accuracy: 0.9486\n",
            "Epoch 6/25\n",
            "110/110 [==============================] - 29s 266ms/step - loss: 4.3263 - accuracy: 0.9549 - val_loss: 10.1627 - val_accuracy: 0.8432\n",
            "Epoch 7/25\n",
            "110/110 [==============================] - 29s 266ms/step - loss: 4.2673 - accuracy: 0.9612 - val_loss: 6.7496 - val_accuracy: 0.9049\n",
            "Epoch 8/25\n",
            "110/110 [==============================] - 29s 266ms/step - loss: 4.3566 - accuracy: 0.9549 - val_loss: 4.4998 - val_accuracy: 0.9614\n",
            "Epoch 9/25\n",
            "110/110 [==============================] - 29s 265ms/step - loss: 3.3869 - accuracy: 0.9594 - val_loss: 3.9495 - val_accuracy: 0.9692\n",
            "Epoch 10/25\n",
            "110/110 [==============================] - 29s 265ms/step - loss: 3.8836 - accuracy: 0.9583 - val_loss: 3.9778 - val_accuracy: 0.9177\n",
            "Epoch 11/25\n",
            "110/110 [==============================] - 29s 268ms/step - loss: 2.5183 - accuracy: 0.9603 - val_loss: 4.2419 - val_accuracy: 0.9717\n",
            "Epoch 12/25\n",
            "110/110 [==============================] - 29s 268ms/step - loss: 3.1349 - accuracy: 0.9566 - val_loss: 3.4044 - val_accuracy: 0.9794\n",
            "Epoch 13/25\n",
            "110/110 [==============================] - 29s 266ms/step - loss: 2.8191 - accuracy: 0.9640 - val_loss: 3.3254 - val_accuracy: 0.9434\n",
            "Epoch 14/25\n",
            "110/110 [==============================] - 29s 267ms/step - loss: 3.5440 - accuracy: 0.9594 - val_loss: 4.3116 - val_accuracy: 0.9794\n",
            "Epoch 15/25\n",
            "110/110 [==============================] - 29s 267ms/step - loss: 3.6006 - accuracy: 0.9617 - val_loss: 3.9827 - val_accuracy: 0.9666\n",
            "Epoch 16/25\n",
            "110/110 [==============================] - 29s 268ms/step - loss: 2.3576 - accuracy: 0.9629 - val_loss: 3.2260 - val_accuracy: 0.9640\n",
            "Epoch 17/25\n",
            "110/110 [==============================] - 30s 271ms/step - loss: 2.1782 - accuracy: 0.9609 - val_loss: 3.7245 - val_accuracy: 0.9743\n",
            "Epoch 18/25\n",
            "110/110 [==============================] - 30s 272ms/step - loss: 2.9873 - accuracy: 0.9649 - val_loss: 4.6930 - val_accuracy: 0.9846\n",
            "Epoch 19/25\n",
            "110/110 [==============================] - 30s 269ms/step - loss: 2.5978 - accuracy: 0.9634 - val_loss: 4.4616 - val_accuracy: 0.9820\n",
            "Epoch 20/25\n",
            "110/110 [==============================] - 30s 274ms/step - loss: 2.6771 - accuracy: 0.9634 - val_loss: 3.9527 - val_accuracy: 0.9820\n",
            "Epoch 21/25\n",
            "110/110 [==============================] - 30s 271ms/step - loss: 2.8140 - accuracy: 0.9612 - val_loss: 2.8767 - val_accuracy: 0.9640\n",
            "Epoch 22/25\n",
            "110/110 [==============================] - 29s 267ms/step - loss: 2.7233 - accuracy: 0.9643 - val_loss: 2.8871 - val_accuracy: 0.9846\n",
            "Epoch 23/25\n",
            "110/110 [==============================] - 29s 267ms/step - loss: 3.0814 - accuracy: 0.9652 - val_loss: 2.1677 - val_accuracy: 0.9846\n",
            "Epoch 24/25\n",
            "110/110 [==============================] - 30s 270ms/step - loss: 2.4062 - accuracy: 0.9663 - val_loss: 3.6749 - val_accuracy: 0.9666\n",
            "Epoch 25/25\n",
            "110/110 [==============================] - 29s 264ms/step - loss: 2.8555 - accuracy: 0.9623 - val_loss: 2.6475 - val_accuracy: 0.9717\n",
            "(1535, 64, 64) (1535, 12)\n",
            "trainX.shape (1381, 64, 64)\n",
            "trainY.shape (1381, 12)\n",
            "testX.shape (154, 64, 64)\n",
            "testY.shape (154, 12)\n",
            "[INFO] compiling model...\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 60, 60, 20)        520       \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 60, 60, 20)       80        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 30, 30, 20)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 26, 26, 50)        25050     \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 26, 26, 50)       200       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 13, 13, 50)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 8450)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1024)              8653824   \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 12)                12300     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,696,070\n",
            "Trainable params: 8,693,882\n",
            "Non-trainable params: 2,188\n",
            "_________________________________________________________________\n",
            "None\n",
            "[INFO] training model...\n",
            "Epoch 1/25\n",
            "44/44 [==============================] - 13s 271ms/step - loss: 481.8508 - accuracy: 0.8878 - val_loss: 5667.6860 - val_accuracy: 0.3247\n",
            "Epoch 2/25\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 9.3381 - accuracy: 1.0000 - val_loss: 723.7532 - val_accuracy: 1.0000\n",
            "Epoch 3/25\n",
            "44/44 [==============================] - 12s 267ms/step - loss: 5.5559 - accuracy: 1.0000 - val_loss: 379.0186 - val_accuracy: 0.9935\n",
            "Epoch 4/25\n",
            "44/44 [==============================] - 12s 267ms/step - loss: 9.1570 - accuracy: 1.0000 - val_loss: 168.8533 - val_accuracy: 1.0000\n",
            "Epoch 5/25\n",
            "44/44 [==============================] - 12s 270ms/step - loss: 5.3436 - accuracy: 1.0000 - val_loss: 38.3777 - val_accuracy: 1.0000\n",
            "Epoch 6/25\n",
            "44/44 [==============================] - 12s 269ms/step - loss: 5.2789 - accuracy: 1.0000 - val_loss: 37.8130 - val_accuracy: 1.0000\n",
            "Epoch 7/25\n",
            "44/44 [==============================] - 12s 268ms/step - loss: 4.2239 - accuracy: 1.0000 - val_loss: 24.9113 - val_accuracy: 1.0000\n",
            "Epoch 8/25\n",
            "44/44 [==============================] - 12s 267ms/step - loss: 4.7394 - accuracy: 1.0000 - val_loss: 13.9760 - val_accuracy: 1.0000\n",
            "Epoch 9/25\n",
            "44/44 [==============================] - 12s 268ms/step - loss: 2.9050 - accuracy: 1.0000 - val_loss: 2.0968 - val_accuracy: 1.0000\n",
            "Epoch 10/25\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 4.5135 - accuracy: 1.0000 - val_loss: 1.9932 - val_accuracy: 1.0000\n",
            "Epoch 11/25\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 2.3072 - accuracy: 1.0000 - val_loss: 3.4474 - val_accuracy: 1.0000\n",
            "Epoch 12/25\n",
            "44/44 [==============================] - 12s 266ms/step - loss: 2.8031 - accuracy: 1.0000 - val_loss: 0.7428 - val_accuracy: 1.0000\n",
            "Epoch 13/25\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 2.0880 - accuracy: 1.0000 - val_loss: 4.0012 - val_accuracy: 1.0000\n",
            "Epoch 14/25\n",
            "44/44 [==============================] - 12s 264ms/step - loss: 2.5598 - accuracy: 1.0000 - val_loss: 1.7209 - val_accuracy: 1.0000\n",
            "Epoch 15/25\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 2.7317 - accuracy: 1.0000 - val_loss: 2.4865 - val_accuracy: 1.0000\n",
            "Epoch 16/25\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 1.6353 - accuracy: 1.0000 - val_loss: 0.9408 - val_accuracy: 1.0000\n",
            "Epoch 17/25\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 2.4591 - accuracy: 1.0000 - val_loss: 1.7022 - val_accuracy: 1.0000\n",
            "Epoch 18/25\n",
            "44/44 [==============================] - 12s 269ms/step - loss: 2.2488 - accuracy: 1.0000 - val_loss: 1.3743 - val_accuracy: 1.0000\n",
            "Epoch 19/25\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 2.5703 - accuracy: 1.0000 - val_loss: 4.9906 - val_accuracy: 1.0000\n",
            "Epoch 20/25\n",
            "44/44 [==============================] - 12s 266ms/step - loss: 2.2823 - accuracy: 1.0000 - val_loss: 1.3899 - val_accuracy: 1.0000\n",
            "Epoch 21/25\n",
            "44/44 [==============================] - 12s 267ms/step - loss: 1.8799 - accuracy: 1.0000 - val_loss: 0.8249 - val_accuracy: 1.0000\n",
            "Epoch 22/25\n",
            "44/44 [==============================] - 12s 267ms/step - loss: 1.8981 - accuracy: 1.0000 - val_loss: 1.1376 - val_accuracy: 1.0000\n",
            "Epoch 23/25\n",
            "44/44 [==============================] - 12s 268ms/step - loss: 1.9645 - accuracy: 1.0000 - val_loss: 1.2558 - val_accuracy: 1.0000\n",
            "Epoch 24/25\n",
            "44/44 [==============================] - 12s 269ms/step - loss: 2.2734 - accuracy: 1.0000 - val_loss: 2.0153 - val_accuracy: 1.0000\n",
            "Epoch 25/25\n",
            "44/44 [==============================] - 12s 268ms/step - loss: 2.3424 - accuracy: 1.0000 - val_loss: 2.4005 - val_accuracy: 1.0000\n",
            "(4120, 64, 64) (4120, 12)\n",
            "trainX.shape (3708, 64, 64)\n",
            "trainY.shape (3708, 12)\n",
            "testX.shape (412, 64, 64)\n",
            "testY.shape (412, 12)\n",
            "[INFO] compiling model...\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_8 (Conv2D)           (None, 60, 60, 20)        520       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 60, 60, 20)       80        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 30, 30, 20)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 26, 26, 50)        25050     \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 26, 26, 50)       200       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 13, 13, 50)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 8450)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1024)              8653824   \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 12)                12300     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,696,070\n",
            "Trainable params: 8,693,882\n",
            "Non-trainable params: 2,188\n",
            "_________________________________________________________________\n",
            "None\n",
            "[INFO] training model...\n",
            "Epoch 1/25\n",
            "116/116 [==============================] - 32s 269ms/step - loss: 212.7726 - accuracy: 0.9471 - val_loss: 225.8158 - val_accuracy: 0.6311\n",
            "Epoch 2/25\n",
            "116/116 [==============================] - 31s 268ms/step - loss: 8.0675 - accuracy: 0.9989 - val_loss: 69.4071 - val_accuracy: 0.7864\n",
            "Epoch 3/25\n",
            "116/116 [==============================] - 31s 268ms/step - loss: 4.7404 - accuracy: 0.9981 - val_loss: 6.8432 - val_accuracy: 0.9612\n",
            "Epoch 4/25\n",
            "116/116 [==============================] - 31s 268ms/step - loss: 4.9323 - accuracy: 0.9976 - val_loss: 8.4133 - val_accuracy: 0.9951\n",
            "Epoch 5/25\n",
            "116/116 [==============================] - 31s 268ms/step - loss: 4.6608 - accuracy: 0.9970 - val_loss: 7.2312 - val_accuracy: 0.9951\n",
            "Epoch 6/25\n",
            "116/116 [==============================] - 31s 268ms/step - loss: 3.4114 - accuracy: 0.9987 - val_loss: 9.8414 - val_accuracy: 1.0000\n",
            "Epoch 7/25\n",
            "116/116 [==============================] - 31s 269ms/step - loss: 3.3789 - accuracy: 0.9987 - val_loss: 9.7530 - val_accuracy: 0.9951\n",
            "Epoch 8/25\n",
            "116/116 [==============================] - 31s 269ms/step - loss: 2.8709 - accuracy: 0.9978 - val_loss: 7.5268 - val_accuracy: 0.9951\n",
            "Epoch 9/25\n",
            "116/116 [==============================] - 31s 269ms/step - loss: 3.0975 - accuracy: 0.9978 - val_loss: 8.6124 - val_accuracy: 0.9951\n",
            "Epoch 10/25\n",
            "116/116 [==============================] - 31s 269ms/step - loss: 4.0505 - accuracy: 0.9973 - val_loss: 14.4871 - val_accuracy: 0.9951\n",
            "Epoch 11/25\n",
            "116/116 [==============================] - 31s 270ms/step - loss: 3.6106 - accuracy: 0.9992 - val_loss: 12.6514 - val_accuracy: 0.9951\n",
            "Epoch 12/25\n",
            "116/116 [==============================] - 31s 268ms/step - loss: 2.7469 - accuracy: 0.9984 - val_loss: 17.6423 - val_accuracy: 0.9951\n",
            "Epoch 13/25\n",
            "116/116 [==============================] - 31s 268ms/step - loss: 3.3267 - accuracy: 0.9992 - val_loss: 10.9880 - val_accuracy: 0.9951\n",
            "Epoch 14/25\n",
            "116/116 [==============================] - 31s 268ms/step - loss: 2.9798 - accuracy: 0.9981 - val_loss: 11.1017 - val_accuracy: 0.9951\n",
            "Epoch 15/25\n",
            "116/116 [==============================] - 31s 268ms/step - loss: 3.1486 - accuracy: 0.9981 - val_loss: 12.6630 - val_accuracy: 0.9951\n",
            "Epoch 16/25\n",
            "116/116 [==============================] - 31s 268ms/step - loss: 3.3225 - accuracy: 0.9989 - val_loss: 10.5166 - val_accuracy: 0.9951\n",
            "Epoch 17/25\n",
            "116/116 [==============================] - 31s 267ms/step - loss: 2.8031 - accuracy: 0.9989 - val_loss: 14.0156 - val_accuracy: 0.9951\n",
            "Epoch 18/25\n",
            "116/116 [==============================] - 31s 267ms/step - loss: 2.8510 - accuracy: 0.9976 - val_loss: 12.4875 - val_accuracy: 0.9951\n",
            "Epoch 19/25\n",
            "116/116 [==============================] - 31s 267ms/step - loss: 3.0022 - accuracy: 0.9984 - val_loss: 9.1053 - val_accuracy: 0.9951\n",
            "Epoch 20/25\n",
            "116/116 [==============================] - 31s 267ms/step - loss: 2.9951 - accuracy: 0.9995 - val_loss: 11.3443 - val_accuracy: 0.9951\n",
            "Epoch 21/25\n",
            "116/116 [==============================] - 31s 268ms/step - loss: 2.9056 - accuracy: 0.9987 - val_loss: 11.0758 - val_accuracy: 0.9951\n",
            "Epoch 22/25\n",
            "116/116 [==============================] - 31s 266ms/step - loss: 2.6427 - accuracy: 0.9995 - val_loss: 11.0171 - val_accuracy: 0.9951\n",
            "Epoch 23/25\n",
            "116/116 [==============================] - 31s 266ms/step - loss: 2.5696 - accuracy: 0.9995 - val_loss: 10.9525 - val_accuracy: 0.9951\n",
            "Epoch 24/25\n",
            "116/116 [==============================] - 31s 267ms/step - loss: 2.3920 - accuracy: 0.9992 - val_loss: 13.8387 - val_accuracy: 0.9951\n",
            "Epoch 25/25\n",
            "116/116 [==============================] - 31s 268ms/step - loss: 2.6522 - accuracy: 1.0000 - val_loss: 15.4200 - val_accuracy: 0.9951\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Nov 30 12:09:46 2020\n",
        "\n",
        "@author: scholar1\n",
        "\"\"\"\n",
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# import the necessary packages\n",
        "#from pyimagesearch.minivggnet import MiniVGGNet\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras import backend as K\n",
        "from imutils import build_montages\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import glob \n",
        "!pip install pywavefront\n",
        "#!apt-get install PyWavwfront\n",
        "import pywavefront\n",
        "import sys\n",
        "\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)\n",
        "\n",
        "def read_point_file(file_path):\n",
        "\tpoints = []\n",
        "\twith open(file_path) as f:\n",
        "\t\tcontent = f.readlines()\n",
        "\n",
        "\t\tfor line in content:\n",
        "\t\t\telements = line.split()\n",
        "\t\t\tpoints.append([float(x) for x in elements])\n",
        "\n",
        "\treturn points\n",
        "\n",
        "def get_3d_control_points_coord_for_all_parts(data_dir):\n",
        "\tannotation_3d_points = np.array(read_point_file(os.path.join(path_to_data, \"markers3dPoints.txt\")))\n",
        "\n",
        "\t# extract only 4 representative parts\n",
        "\tannotation_3d_points = np.delete(annotation_3d_points, [1, 3, 5, 7], 0)\n",
        "\tassert annotation_3d_points.shape == (16, 3)\n",
        "\n",
        "\tcontrol_point_offset = np.array([[0.05, 0, 0], [-0.05, 0, 0], [0, 0.05, 0], [0, -0.05, 0], [0, 0, 0.05], [0, 0, -0.05]])\n",
        "\n",
        "\tcontrol_points_for_all_parts = []\n",
        "\tfor annotation_pt in annotation_3d_points:\n",
        "\t\tcontrol_pts_for_part = []\n",
        "\t\tfor offset in control_point_offset:\n",
        "\t\t\tcontrol_pts_for_part.append(annotation_pt + offset)\n",
        "\n",
        "\t\tcontrol_points_for_all_parts.append(control_pts_for_part)\n",
        "\n",
        "\tcontrol_points_for_all_parts = np.array(control_points_for_all_parts)\n",
        "\t#assert control_points_for_all_parts.shape == (4, 6, 3)\n",
        "\n",
        "\treturn control_points_for_all_parts\n",
        "\n",
        "def difference_of_Gaussians(img):\n",
        "\tkernel1 = cv2.getGaussianKernel(10, 1)\n",
        "\tkernel2 = cv2.getGaussianKernel(10, 3)\n",
        "\n",
        "\tdog_img = cv2.sepFilter2D(img, -1, kernel1 - kernel2, kernel1 - kernel2)\n",
        "\tdog_img = cv2.normalize(dog_img, dog_img, alpha = 0, beta = 255, norm_type=cv2.NORM_MINMAX)\n",
        "\n",
        "\treturn dog_img\n",
        "\n",
        "def get_cropped_image(x, y, img):\n",
        "\tx_min = x - 16\n",
        "\tx_max = x + 16\n",
        "\ty_min = y - 16\n",
        "\ty_max = y + 16\n",
        "\tif x_min < 0 or x_max >= np.size(img,1) or y_min < 0 or y_max >= np.size(img,0):\n",
        "\t\treturn np.empty(0)\n",
        "\n",
        "\t# crop = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)[y_min:y_max, x_min:x_max]\n",
        "\tcrop = difference_of_Gaussians(img[y_min:y_max, x_min:x_max]) # this was changed! check the latest extract_parts.py\n",
        "\n",
        "\treturn crop\n",
        "\n",
        "labelNames = [\"0\", \"1\", \"2\", \"3\", \"background\"]\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/video1/training/\"\n",
        "\n",
        "\n",
        "mesh = pywavefront.Wavefront(os.path.join(\"/content/drive/MyDrive/video1/openbox.obj\"))\n",
        "\n",
        "for name, material in mesh.materials.items():\n",
        " \ttriangles = material.vertices\n",
        "\n",
        "triangles = np.array(triangles)\n",
        "triangles = triangles.reshape(-1, 3)\n",
        "\n",
        "#test from training data\n",
        "path_to_data = \"/content/drive/MyDrive/video1test/\"\n",
        "\n",
        "\n",
        "control_points_for_all_parts = get_3d_control_points_coord_for_all_parts(data_dir)\n",
        "\n",
        "#path_to_data = os.path.join(data_dir, \"3D Rigid Tracking from RGB Images Dataset/BOX-TestAndInfo/Test/video1\")\n",
        "\n",
        "\n",
        "\n",
        "num_input_files = len(glob.glob(os.path.join(path_to_data,\"*.png\")))\n",
        "\n",
        "camera_matrix = np.float32([[2666.67, 0, 960], [0, 2666.67, 540], [0, 0, 1.]])\n",
        "\n",
        "model = tf.keras.models.load_model('model_part_detection.h5')\n",
        "\n",
        "model_for_control_points = []\n",
        "\n",
        "for part_id in range(4):\n",
        "\tmodel_for_control_points.append(tf.keras.models.load_model(\"model_control_points_\" + str(part_id) + \".h5\"))\n",
        "\n",
        "n = 0\n",
        "\n",
        "while True:\n",
        "\tprint(\"Pose estimation:\" + str(n))\n",
        "\ttestX = []\n",
        "\ttest_locations = []\n",
        "\n",
        "\tfile_name = \"frame\" + str(n).zfill(5) + \".png\"\n",
        "\timg_path = os.path.join(path_to_data, file_name)\n",
        "\timg = cv2.imread(img_path)\n",
        "\timg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    \n",
        "     \n",
        "\t# scale image by the same ratio when generating training data\n",
        "\tscale_ratio = 0.25\n",
        "\tinv_scale_ratio = 1.0 / scale_ratio\n",
        "\n",
        "\timg = cv2.resize(img, None, fx=scale_ratio, fy=scale_ratio, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\tnormalize = 1.0 / 255\n",
        "\tstep = 2\n",
        "\n",
        "\tfor y in range(16, img.shape[0]-16, step):\n",
        "\t\tfor x in range(16, img.shape[1]-16, step):\n",
        "\t\t\tcropped = get_cropped_image(x, y, img)\n",
        "\t\t\tif cropped.shape == 0:\n",
        "\t\t\t\tcontinue;\n",
        "\t\t\t\t\n",
        "\t\t\t# scale data to the range of [0, 1]\n",
        "\t\t\tcropped = cropped.astype(\"float32\") * normalize # normalize outside of loop!\n",
        "\n",
        "\t\t\ttestX.append(cropped)\n",
        "\t\t\ttest_locations.append((x, y))\n",
        "\n",
        "\tprint(\"data ready\")\n",
        "\n",
        "\ttest_locations_width = int((img.shape[1] - 32) / step)\n",
        "\ttest_locations_height = int((img.shape[0] - 32) / step)\n",
        "\n",
        "\t# convert from python list to numpy array\n",
        "\ttestX = np.array(testX)\n",
        "\n",
        "\tif K.image_data_format() == \"channels_first\":\n",
        "\t\ttestX = testX.reshape((testX.shape[0], 1, 32, 32))\n",
        "\telse:\n",
        "\t\ttestX = testX.reshape((testX.shape[0], 32, 32, 1))\n",
        "\n",
        "\tprint(\"testX.shape\", testX.shape)\n",
        "\n",
        "\t# use 1st CNN to a set of 32x32 patches\n",
        "\t#probs[i]: probability of i-th patch, which part the patch is?\n",
        "\tprobs = model.predict(testX)\n",
        "\n",
        "\ttest_locations_probs = probs.reshape(test_locations_height, test_locations_width, probs.shape[1])\n",
        "\n",
        "\tto_test_location = lambda coord : coord * step + np.array([16, 16])\n",
        "                    \n",
        "\tlabel_color = [ (0,0,255), (0,255,0), (255,0,0), (0,255,255), ]\n",
        "\tcolor_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "\tpart_probability = []\n",
        "\tobject_points_for_pose_estimation = []\n",
        "\timage_points_for_pose_estimation = []\n",
        "\n",
        "\t# apply each 2nd CNN to a set of 64x64 patches\n",
        "\tfor part_id in range(4):\n",
        "\t\ttest_locations_probs_for_part = test_locations_probs[:,:,part_id]\n",
        "\n",
        "\t\tretval, high_probs_locations = cv2.threshold(test_locations_probs_for_part, 0.99, 255, cv2.THRESH_BINARY)\n",
        "\t\thigh_probs_locations = high_probs_locations.astype(\"ubyte\")\n",
        "\n",
        "\t\tretval, labels, stats, centroids = cv2.connectedComponentsWithStats(high_probs_locations, connectivity=8)\n",
        "\n",
        "\t\tkMinRequiredArea = 3\n",
        "\t\tpart_location_candidates = []\n",
        "\n",
        "\t\tfor label in range(1, stats.shape[0]):\n",
        "\t\t\tarea = stats[label, cv2.CC_STAT_AREA]\n",
        "\t\t\tcentorid = np.array([int(centroids[label, 0]), int(centroids[label, 1])])\n",
        "\n",
        "\t\t\tpart_location_candidates.append((to_test_location(centorid), area))\n",
        "\t\t\n",
        "\t\tpart_location_candidates.sort(key=lambda candidate: candidate[1], reverse = True)\n",
        "\n",
        "\t\tif part_location_candidates and part_location_candidates[0][1] >= kMinRequiredArea:\n",
        "\t\t\tlocation = part_location_candidates[0][0]\n",
        "\t\t\tcv2.circle(color_img, tuple(location), 10, label_color[part_id], 1)\n",
        "\n",
        "\t\t\tx_min = location[0] - 32\n",
        "\t\t\tx_max = location[0] + 32\n",
        "\t\t\ty_min = location[1] - 32\n",
        "\t\t\ty_max = location[1] + 32\n",
        "\t\t\tif x_min < 0 or x_max >= np.size(img,1) or y_min < 0 or y_max >= np.size(img,0):\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\tcrop_for_control_points = img[y_min:y_max, x_min:x_max]\n",
        "\n",
        "\t\t\t# scale data to the range of [0, 1]\n",
        "\t\t\tcrop_for_control_points = crop_for_control_points.astype(\"float32\") * normalize # normalize outside of loop!\n",
        "\n",
        "\t\t\t# convert from python list to numpy array\n",
        "\t\t\ttestX = np.array([crop_for_control_points])\n",
        "\n",
        "\t\t\tif K.image_data_format() == \"channels_first\":\n",
        "\t\t\t\ttestX = testX.reshape((testX.shape[0], 1, 64, 64))\n",
        "\t\t\telse:\n",
        "\t\t\t\ttestX = testX.reshape((testX.shape[0], 64, 64, 1))\n",
        "\n",
        "\t\t\tprint(\"testX.shape\", testX.shape)\n",
        "\n",
        "\t\t\t# use 2nd CNN to estimate pose of the control part\n",
        "\t\t\tcontrol_points_offset = model_for_control_points[part_id].predict(testX)\n",
        "\t\t\tcontrol_points_offset = control_points_offset.reshape(6, 1, 2)\n",
        "\n",
        "\t\t\tctrl_pt_color = [ (0,0,255), (0,0,255), (0,255,0), (0,255,0), (255,0,0), (255,0,0) ]\n",
        "\n",
        "\t\t\tif True: # visualize control points\n",
        "\t\t\t\tfor idx, offset in enumerate(control_points_offset):\n",
        "\t\t\t\t\tprojected_control_points = location + offset * 0.25 #32 + offset * 0.25\n",
        "\t\t\t\t\tcv2.circle(color_img, (int(projected_control_points[0][0]), int(projected_control_points[0][1])), 3, ctrl_pt_color[idx], -1)\n",
        "\n",
        "\t\t\tobject_points_for_pose_estimation.append(control_points_for_all_parts[part_id])\n",
        "\t\t\timage_points_for_pose_estimation.append(location * inv_scale_ratio + control_points_offset)\n",
        "\n",
        "\t# compute object pose\n",
        "\tif object_points_for_pose_estimation and image_points_for_pose_estimation:\n",
        "\n",
        "\t\tobject_points_for_pose_estimation = np.array(object_points_for_pose_estimation).reshape(-1, 3)\n",
        "\t\timage_points_for_pose_estimation = np.array(image_points_for_pose_estimation).reshape(-1, 2)\n",
        "        \n",
        "        # output on values to be researched \n",
        "\t\tretval, rvec, tvec, inliers\t= cv2.solvePnPRansac(object_points_for_pose_estimation, image_points_for_pose_estimation, camera_matrix, np.empty(0))\n",
        "\t\t#print(retval)\n",
        "\t\tprint(rvec)\n",
        "\t\tprint(tvec)\n",
        "\t\t#print(inliers)\n",
        "\n",
        "\t\tto_tuple = lambda p : (int(p[0][0] * scale_ratio), int(p[0][1] * scale_ratio))\n",
        "        \n",
        "        #search what it means\n",
        "\t\tprojected_triangle_points, jac = cv2.projectPoints(triangles, rvec, tvec, camera_matrix, np.empty(0))\n",
        "\t\tfor j in range(0, projected_triangle_points.shape[0], 3):\n",
        " \t\t\tcv2.line(color_img, to_tuple(projected_triangle_points[j]), to_tuple(projected_triangle_points[j+1]), (255, 255, 100))\n",
        " \t\t\tcv2.line(color_img, to_tuple(projected_triangle_points[j+1]), to_tuple(projected_triangle_points[j+2]), (255, 255, 100))\n",
        " \t\t\tcv2.line(color_img, to_tuple(projected_triangle_points[j+2]), to_tuple(projected_triangle_points[j]), (255, 255, 100))\n",
        "\n",
        "\tcv2.putText(color_img, str(n), (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "\tprint(\"done\")\n",
        "\n",
        "\tif True: # write images for video\n",
        "\t\tprint(color_img.shape)\n",
        "        # image outputs tracking\n",
        "\t\tcv2.imwrite(\"/content/drive/MyDrive/output/Pose estimation\" + str(n) + \".jpg\", color_img)\n",
        "\t\tprint(str(n) + \" / \" + str(num_input_files))\n",
        "\t\tn = n + 1\n",
        "\t\tif n == num_input_files:\n",
        "\t\t\tsys.exit(1)\n",
        "\t\tcontinue\n",
        "\n",
        "\tcv2.imshow(\"Pose estimation\", color_img)\n",
        "\tkey = cv2.waitKey(1)\n",
        "\n",
        "\tprint(\"key\", key)\n",
        "\n",
        "\tif key == 27:         # wait for ESC key to exit\n",
        "\t\tcv2.destroyAllWindows()\n",
        "\t\tsys.exit(0)\n",
        "\telif key == 81: # left arrow\n",
        "\t\tn = max(n - 1, 0)\n",
        "\telif key == 82: # up arrow\n",
        "\t\tn = n + 5\n",
        "\telif key == 84: # down arrow\n",
        "\t\tn = n - 5\n",
        "\telse:\n",
        "\t\tn = n + 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s5vfW8-weOl5",
        "outputId": "57d9831a-6f1d-4711-9796-e89e6af02840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pywavefront\n",
            "  Downloading PyWavefront-1.3.3-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: pywavefront\n",
            "Successfully installed pywavefront-1.3.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1768: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "Unimplemented OBJ format statement 'g' on line 'g group1'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pose estimation:0\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.71656116]\n",
            " [ 0.01246397]\n",
            " [-0.05078194]]\n",
            "[[-0.10306378]\n",
            " [ 0.14260858]\n",
            " [ 1.21016429]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "0 / 201\n",
            "Pose estimation:1\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.71287415]\n",
            " [ 0.0029823 ]\n",
            " [-0.06775319]]\n",
            "[[-0.10321784]\n",
            " [ 0.14464516]\n",
            " [ 1.20582698]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "1 / 201\n",
            "Pose estimation:2\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.70816397e+00]\n",
            " [ 4.21291464e-04]\n",
            " [-7.73437918e-02]]\n",
            "[[-0.10390385]\n",
            " [ 0.14408975]\n",
            " [ 1.20568615]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "2 / 201\n",
            "Pose estimation:3\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "3 / 201\n",
            "Pose estimation:4\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "4 / 201\n",
            "Pose estimation:5\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "5 / 201\n",
            "Pose estimation:6\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "6 / 201\n",
            "Pose estimation:7\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "7 / 201\n",
            "Pose estimation:8\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "8 / 201\n",
            "Pose estimation:9\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "9 / 201\n",
            "Pose estimation:10\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "10 / 201\n",
            "Pose estimation:11\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "11 / 201\n",
            "Pose estimation:12\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.71819265]\n",
            " [-0.00584219]\n",
            " [-0.08319764]]\n",
            "[[-0.10564078]\n",
            " [ 0.14577065]\n",
            " [ 1.22688302]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "12 / 201\n",
            "Pose estimation:13\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.71692489]\n",
            " [ 0.02047154]\n",
            " [-0.04304088]]\n",
            "[[-0.10726804]\n",
            " [ 0.13834403]\n",
            " [ 1.22710532]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "13 / 201\n",
            "Pose estimation:14\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.71033389]\n",
            " [-0.00439883]\n",
            " [-0.05871685]]\n",
            "[[-0.10323222]\n",
            " [ 0.14772975]\n",
            " [ 1.21277074]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "14 / 201\n",
            "Pose estimation:15\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.7127914 ]\n",
            " [-0.01709288]\n",
            " [-0.06927188]]\n",
            "[[-0.09513312]\n",
            " [ 0.15300772]\n",
            " [ 1.2318432 ]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "15 / 201\n",
            "Pose estimation:16\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.71622316]\n",
            " [-0.04958743]\n",
            " [-0.06009237]]\n",
            "[[-0.07886578]\n",
            " [ 0.16274678]\n",
            " [ 1.22974448]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "16 / 201\n",
            "Pose estimation:17\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.71036867]\n",
            " [-0.00592895]\n",
            " [-0.09879848]]\n",
            "[[-0.09761628]\n",
            " [ 0.15995037]\n",
            " [ 1.24358309]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "17 / 201\n",
            "Pose estimation:18\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.70610642e+00]\n",
            " [-7.49971492e-04]\n",
            " [-1.11913585e-01]]\n",
            "[[-0.09649318]\n",
            " [ 0.16283602]\n",
            " [ 1.24608601]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "18 / 201\n",
            "Pose estimation:19\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.70141008]\n",
            " [ 0.0290766 ]\n",
            " [-0.08736454]]\n",
            "[[-0.09804242]\n",
            " [ 0.15600522]\n",
            " [ 1.23775841]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "19 / 201\n",
            "Pose estimation:20\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69729145]\n",
            " [ 0.00713147]\n",
            " [-0.11963855]]\n",
            "[[-0.09869812]\n",
            " [ 0.16834263]\n",
            " [ 1.21071707]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "20 / 201\n",
            "Pose estimation:21\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.70405435]\n",
            " [ 0.03745039]\n",
            " [-0.10888923]]\n",
            "[[-0.10316991]\n",
            " [ 0.16033303]\n",
            " [ 1.24751185]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "21 / 201\n",
            "Pose estimation:22\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69728171]\n",
            " [ 0.05190283]\n",
            " [-0.1465294 ]]\n",
            "[[-0.11721444]\n",
            " [ 0.1617298 ]\n",
            " [ 1.25730528]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "22 / 201\n",
            "Pose estimation:23\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69786588]\n",
            " [ 0.04259941]\n",
            " [-0.12139282]]\n",
            "[[-0.11139328]\n",
            " [ 0.16284736]\n",
            " [ 1.24744569]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "23 / 201\n",
            "Pose estimation:24\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69957696]\n",
            " [ 0.03963933]\n",
            " [-0.11356996]]\n",
            "[[-0.10917239]\n",
            " [ 0.1633879 ]\n",
            " [ 1.24715945]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "24 / 201\n",
            "Pose estimation:25\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69897287]\n",
            " [ 0.03821864]\n",
            " [-0.10977457]]\n",
            "[[-0.10807156]\n",
            " [ 0.16317439]\n",
            " [ 1.24687402]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "25 / 201\n",
            "Pose estimation:26\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69844935]\n",
            " [ 0.0431366 ]\n",
            " [-0.12145865]]\n",
            "[[-0.11097965]\n",
            " [ 0.16241146]\n",
            " [ 1.25151275]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "26 / 201\n",
            "Pose estimation:27\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69649246]\n",
            " [ 0.01406418]\n",
            " [-0.14297258]]\n",
            "[[-0.10532739]\n",
            " [ 0.17220677]\n",
            " [ 1.25291139]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "27 / 201\n",
            "Pose estimation:28\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.68433441]\n",
            " [ 0.07805708]\n",
            " [-0.11589523]]\n",
            "[[-0.12125717]\n",
            " [ 0.1524198 ]\n",
            " [ 1.24565763]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "28 / 201\n",
            "Pose estimation:29\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69373049]\n",
            " [ 0.04271217]\n",
            " [-0.11882619]]\n",
            "[[-0.11522776]\n",
            " [ 0.16101143]\n",
            " [ 1.24875207]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "29 / 201\n",
            "Pose estimation:30\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69306335]\n",
            " [ 0.01381146]\n",
            " [-0.14694567]]\n",
            "[[-0.11061436]\n",
            " [ 0.17166026]\n",
            " [ 1.25163998]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "30 / 201\n",
            "Pose estimation:31\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69146186]\n",
            " [ 0.0156778 ]\n",
            " [-0.15182035]]\n",
            "[[-0.11194674]\n",
            " [ 0.17114288]\n",
            " [ 1.2520828 ]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "31 / 201\n",
            "Pose estimation:32\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69189594]\n",
            " [ 0.01913522]\n",
            " [-0.15932291]]\n",
            "[[-0.12132716]\n",
            " [ 0.17553882]\n",
            " [ 1.21996854]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "32 / 201\n",
            "Pose estimation:33\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69312456]\n",
            " [ 0.0136582 ]\n",
            " [-0.14883445]]\n",
            "[[-0.11538024]\n",
            " [ 0.17181907]\n",
            " [ 1.25226981]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "33 / 201\n",
            "Pose estimation:34\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69310207]\n",
            " [ 0.01342038]\n",
            " [-0.14854375]]\n",
            "[[-0.11535986]\n",
            " [ 0.17192885]\n",
            " [ 1.25154903]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "34 / 201\n",
            "Pose estimation:35\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69277534]\n",
            " [ 0.01336475]\n",
            " [-0.1492434 ]]\n",
            "[[-0.11540937]\n",
            " [ 0.17185903]\n",
            " [ 1.25185781]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "35 / 201\n",
            "Pose estimation:36\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69308007]\n",
            " [ 0.01383321]\n",
            " [-0.1492877 ]]\n",
            "[[-0.11553287]\n",
            " [ 0.17183346]\n",
            " [ 1.25213295]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "36 / 201\n",
            "Pose estimation:37\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69400743]\n",
            " [ 0.01420278]\n",
            " [-0.14676181]]\n",
            "[[-0.1152805 ]\n",
            " [ 0.17186062]\n",
            " [ 1.25182845]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "37 / 201\n",
            "Pose estimation:38\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.68550609]\n",
            " [ 0.04909226]\n",
            " [-0.14404109]]\n",
            "[[-0.1257035 ]\n",
            " [ 0.16351723]\n",
            " [ 1.250963  ]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "38 / 201\n",
            "Pose estimation:39\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.68728055]\n",
            " [ 0.01553199]\n",
            " [-0.13919265]]\n",
            "[[-0.12216923]\n",
            " [ 0.17375795]\n",
            " [ 1.21232913]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "39 / 201\n",
            "Pose estimation:40\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.68139609]\n",
            " [ 0.01330181]\n",
            " [-0.13091836]]\n",
            "[[-0.12053722]\n",
            " [ 0.172028  ]\n",
            " [ 1.20757037]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "40 / 201\n",
            "Pose estimation:41\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.68388422]\n",
            " [-0.01436948]\n",
            " [-0.15999167]]\n",
            "[[-0.11320631]\n",
            " [ 0.1785563 ]\n",
            " [ 1.24875704]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "41 / 201\n",
            "Pose estimation:42\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.68653701]\n",
            " [ 0.01506147]\n",
            " [-0.13388639]]\n",
            "[[-0.11838065]\n",
            " [ 0.16849566]\n",
            " [ 1.24708769]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "42 / 201\n",
            "Pose estimation:43\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.68137934]\n",
            " [ 0.01789788]\n",
            " [-0.13978667]]\n",
            "[[-0.12759116]\n",
            " [ 0.17152209]\n",
            " [ 1.2105715 ]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "43 / 201\n",
            "Pose estimation:44\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.6808918 ]\n",
            " [ 0.01795984]\n",
            " [-0.14171933]]\n",
            "[[-0.12796403]\n",
            " [ 0.17155261]\n",
            " [ 1.21041481]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "44 / 201\n",
            "Pose estimation:45\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69008721]\n",
            " [ 0.01866846]\n",
            " [-0.15153922]]\n",
            "[[-0.12655162]\n",
            " [ 0.1700297 ]\n",
            " [ 1.25198046]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "45 / 201\n",
            "Pose estimation:46\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69218412]\n",
            " [ 0.06295709]\n",
            " [-0.1838788 ]]\n",
            "[[-0.14880666]\n",
            " [ 0.16506443]\n",
            " [ 1.26382845]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "46 / 201\n",
            "Pose estimation:47\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69206022]\n",
            " [ 0.02589495]\n",
            " [-0.18114956]]\n",
            "[[-0.13734088]\n",
            " [ 0.17110385]\n",
            " [ 1.25977544]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "47 / 201\n",
            "Pose estimation:48\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69319254]\n",
            " [ 0.03423188]\n",
            " [-0.20423377]]\n",
            "[[-0.15001592]\n",
            " [ 0.17603153]\n",
            " [ 1.2298427 ]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "48 / 201\n",
            "Pose estimation:49\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69966626]\n",
            " [ 0.02995024]\n",
            " [-0.19525899]]\n",
            "[[-0.14472412]\n",
            " [ 0.17336235]\n",
            " [ 1.26610138]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "49 / 201\n",
            "Pose estimation:50\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.68939549]\n",
            " [ 0.03172371]\n",
            " [-0.20018838]]\n",
            "[[-0.14654622]\n",
            " [ 0.17067034]\n",
            " [ 1.26175488]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "50 / 201\n",
            "Pose estimation:51\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.68576254]\n",
            " [ 0.0340814 ]\n",
            " [-0.2060924 ]]\n",
            "[[-0.14828437]\n",
            " [ 0.16966841]\n",
            " [ 1.261102  ]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "51 / 201\n",
            "Pose estimation:52\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69643797]\n",
            " [ 0.03540728]\n",
            " [-0.21162909]]\n",
            "[[-0.15329229]\n",
            " [ 0.17254612]\n",
            " [ 1.26824044]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "52 / 201\n",
            "Pose estimation:53\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69484425e+00]\n",
            " [-9.63292537e-04]\n",
            " [-1.97791849e-01]]\n",
            "[[-0.1404527 ]\n",
            " [ 0.17701259]\n",
            " [ 1.26162949]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "53 / 201\n",
            "Pose estimation:54\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69768935]\n",
            " [ 0.03888432]\n",
            " [-0.21541681]]\n",
            "[[-0.16166024]\n",
            " [ 0.17269046]\n",
            " [ 1.2338452 ]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "54 / 201\n",
            "Pose estimation:55\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.6960524 ]\n",
            " [ 0.03999726]\n",
            " [-0.21862178]]\n",
            "[[-0.16262962]\n",
            " [ 0.17238616]\n",
            " [ 1.23284149]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "55 / 201\n",
            "Pose estimation:56\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69226749]\n",
            " [ 0.04385139]\n",
            " [-0.22926481]]\n",
            "[[-0.16533351]\n",
            " [ 0.17132308]\n",
            " [ 1.23384287]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "56 / 201\n",
            "Pose estimation:57\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.70544858]\n",
            " [ 0.03779114]\n",
            " [-0.2163854 ]]\n",
            "[[-0.15895573]\n",
            " [ 0.17044215]\n",
            " [ 1.27165624]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "57 / 201\n",
            "Pose estimation:58\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69928727]\n",
            " [ 0.04228399]\n",
            " [-0.2269933 ]]\n",
            "[[-0.16204152]\n",
            " [ 0.16869243]\n",
            " [ 1.27093493]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "58 / 201\n",
            "Pose estimation:59\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69530946]\n",
            " [ 0.04473667]\n",
            " [-0.23305859]]\n",
            "[[-0.16377499]\n",
            " [ 0.16758281]\n",
            " [ 1.27048907]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "59 / 201\n",
            "Pose estimation:60\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.6951109 ]\n",
            " [ 0.04403319]\n",
            " [-0.23024626]]\n",
            "[[-0.16325176]\n",
            " [ 0.16750035]\n",
            " [ 1.26949736]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "60 / 201\n",
            "Pose estimation:61\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69900927]\n",
            " [ 0.04555854]\n",
            " [-0.23463899]]\n",
            "[[-0.17062707]\n",
            " [ 0.17317958]\n",
            " [ 1.23793416]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "61 / 201\n",
            "Pose estimation:62\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69829836]\n",
            " [ 0.04604983]\n",
            " [-0.23519098]]\n",
            "[[-0.17086891]\n",
            " [ 0.17291496]\n",
            " [ 1.23787723]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "62 / 201\n",
            "Pose estimation:63\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69870605]\n",
            " [ 0.04389656]\n",
            " [-0.22624015]]\n",
            "[[-0.16909126]\n",
            " [ 0.17281868]\n",
            " [ 1.23589563]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "63 / 201\n",
            "Pose estimation:64\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69237437]\n",
            " [ 0.03626035]\n",
            " [-0.19880536]]\n",
            "[[-0.15671236]\n",
            " [ 0.16578095]\n",
            " [ 1.26318732]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "64 / 201\n",
            "Pose estimation:65\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69276312]\n",
            " [ 0.01412607]\n",
            " [-0.236595  ]]\n",
            "[[-0.16147806]\n",
            " [ 0.18089904]\n",
            " [ 1.23455893]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "65 / 201\n",
            "Pose estimation:66\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.68793817]\n",
            " [ 0.01044077]\n",
            " [-0.22511781]]\n",
            "[[-0.15914252]\n",
            " [ 0.17950162]\n",
            " [ 1.22974086]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "66 / 201\n",
            "Pose estimation:67\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69684716]\n",
            " [ 0.04676422]\n",
            " [-0.24220837]]\n",
            "[[-0.17213782]\n",
            " [ 0.17749641]\n",
            " [ 1.23792784]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "67 / 201\n",
            "Pose estimation:68\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69854727]\n",
            " [ 0.04552048]\n",
            " [-0.23759591]]\n",
            "[[-0.17101946]\n",
            " [ 0.17778106]\n",
            " [ 1.23821209]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "68 / 201\n",
            "Pose estimation:69\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69518331]\n",
            " [ 0.01453023]\n",
            " [-0.25029611]]\n",
            "[[-0.1565766 ]\n",
            " [ 0.18254618]\n",
            " [ 1.2728524 ]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "69 / 201\n",
            "Pose estimation:70\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.70055474]\n",
            " [-0.0096215 ]\n",
            " [-0.17568833]]\n",
            "[[-0.13681691]\n",
            " [ 0.18163159]\n",
            " [ 1.27101352]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "70 / 201\n",
            "Pose estimation:71\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.68815525]\n",
            " [ 0.00450124]\n",
            " [-0.21554356]]\n",
            "[[-0.15077658]\n",
            " [ 0.18394889]\n",
            " [ 1.23180484]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "71 / 201\n",
            "Pose estimation:72\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.70038735]\n",
            " [ 0.04759838]\n",
            " [-0.24734213]]\n",
            "[[-0.16818367]\n",
            " [ 0.18291764]\n",
            " [ 1.24205036]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "72 / 201\n",
            "Pose estimation:73\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69969025]\n",
            " [ 0.03957704]\n",
            " [-0.2216623 ]]\n",
            "[[-0.15570664]\n",
            " [ 0.17788396]\n",
            " [ 1.27168506]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "73 / 201\n",
            "Pose estimation:74\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.70415281]\n",
            " [ 0.03027625]\n",
            " [-0.18693985]]\n",
            "[[-0.14802088]\n",
            " [ 0.17841042]\n",
            " [ 1.26678799]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "74 / 201\n",
            "Pose estimation:75\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.68903471]\n",
            " [ 0.01552825]\n",
            " [-0.24973756]]\n",
            "[[-0.15473433]\n",
            " [ 0.18949963]\n",
            " [ 1.23653501]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "75 / 201\n",
            "Pose estimation:76\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.68877217]\n",
            " [ 0.0103481 ]\n",
            " [-0.23213738]]\n",
            "[[-0.15091936]\n",
            " [ 0.18910983]\n",
            " [ 1.23249719]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "76 / 201\n",
            "Pose estimation:77\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.67104348]\n",
            " [-0.00414198]\n",
            " [-0.27490478]]\n",
            "[[-0.1551822 ]\n",
            " [ 0.19838414]\n",
            " [ 1.19496381]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "77 / 201\n",
            "Pose estimation:78\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.70542196]\n",
            " [ 0.03768018]\n",
            " [-0.22429973]]\n",
            "[[-0.15042569]\n",
            " [ 0.18443958]\n",
            " [ 1.27599872]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "78 / 201\n",
            "Pose estimation:79\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69106128]\n",
            " [ 0.0210259 ]\n",
            " [-0.27831785]]\n",
            "[[-0.15535543]\n",
            " [ 0.19571705]\n",
            " [ 1.24342423]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "79 / 201\n",
            "Pose estimation:80\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.68807628]\n",
            " [ 0.01588178]\n",
            " [-0.26525189]]\n",
            "[[-0.14547212]\n",
            " [ 0.19066638]\n",
            " [ 1.27356281]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "80 / 201\n",
            "Pose estimation:81\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69797366]\n",
            " [ 0.00868133]\n",
            " [-0.24397526]]\n",
            "[[-0.13986645]\n",
            " [ 0.192922  ]\n",
            " [ 1.27453936]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "81 / 201\n",
            "Pose estimation:82\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69021669]\n",
            " [ 0.01387115]\n",
            " [-0.25032643]]\n",
            "[[-0.14527537]\n",
            " [ 0.19465251]\n",
            " [ 1.23737534]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "82 / 201\n",
            "Pose estimation:83\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.68761951]\n",
            " [ 0.01031955]\n",
            " [-0.23956078]]\n",
            "[[-0.13588726]\n",
            " [ 0.18949382]\n",
            " [ 1.26869893]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "83 / 201\n",
            "Pose estimation:84\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.70048008]\n",
            " [ 0.01219716]\n",
            " [-0.25459171]]\n",
            "[[-0.13734409]\n",
            " [ 0.19348416]\n",
            " [ 1.27952477]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "84 / 201\n",
            "Pose estimation:85\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.71080358]\n",
            " [ 0.04324032]\n",
            " [-0.24795336]]\n",
            "[[-0.14521086]\n",
            " [ 0.18609339]\n",
            " [ 1.28648283]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "85 / 201\n",
            "Pose estimation:86\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69617159]\n",
            " [-0.00343063]\n",
            " [-0.19539172]]\n",
            "[[-0.12299814]\n",
            " [ 0.1803618 ]\n",
            " [ 1.27511307]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "86 / 201\n",
            "Pose estimation:87\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.71380263]\n",
            " [ 0.04373042]\n",
            " [-0.24269528]]\n",
            "[[-0.14452698]\n",
            " [ 0.17258248]\n",
            " [ 1.28692344]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "87 / 201\n",
            "Pose estimation:88\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.70709308]\n",
            " [ 0.05282452]\n",
            " [-0.25531698]]\n",
            "[[-0.15222058]\n",
            " [ 0.16601898]\n",
            " [ 1.24909194]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "88 / 201\n",
            "Pose estimation:89\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.70912933]\n",
            " [ 0.02652987]\n",
            " [-0.27650226]]\n",
            "[[-0.1422449 ]\n",
            " [ 0.17215125]\n",
            " [ 1.25200476]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "89 / 201\n",
            "Pose estimation:90\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.71737169]\n",
            " [ 0.05831518]\n",
            " [-0.27348155]]\n",
            "[[-0.15110951]\n",
            " [ 0.16459155]\n",
            " [ 1.25649193]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "90 / 201\n",
            "Pose estimation:91\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.71530925]\n",
            " [ 0.06317032]\n",
            " [-0.28632127]]\n",
            "[[-0.15423334]\n",
            " [ 0.1641028 ]\n",
            " [ 1.25806185]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "91 / 201\n",
            "Pose estimation:92\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.7181521 ]\n",
            " [ 0.06182393]\n",
            " [-0.28286183]]\n",
            "[[-0.15328949]\n",
            " [ 0.16486085]\n",
            " [ 1.25829429]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "92 / 201\n",
            "Pose estimation:93\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.70937362]\n",
            " [ 0.05268378]\n",
            " [-0.23998072]]\n",
            "[[-0.14576161]\n",
            " [ 0.15653319]\n",
            " [ 1.24649736]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "93 / 201\n",
            "Pose estimation:94\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.7174211 ]\n",
            " [ 0.02758788]\n",
            " [-0.27493878]]\n",
            "[[-0.14204193]\n",
            " [ 0.16957937]\n",
            " [ 1.2542025 ]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "94 / 201\n",
            "Pose estimation:95\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.7223313 ]\n",
            " [ 0.05563538]\n",
            " [-0.2627054 ]]\n",
            "[[-0.14908539]\n",
            " [ 0.16145024]\n",
            " [ 1.25396024]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "95 / 201\n",
            "Pose estimation:96\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72445266]\n",
            " [ 0.05632552]\n",
            " [-0.26698675]]\n",
            "[[-0.14964027]\n",
            " [ 0.16214556]\n",
            " [ 1.2559682 ]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "96 / 201\n",
            "Pose estimation:97\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.71837243]\n",
            " [ 0.04643143]\n",
            " [-0.22596232]]\n",
            "[[-0.1419281 ]\n",
            " [ 0.15499775]\n",
            " [ 1.24520847]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "97 / 201\n",
            "Pose estimation:98\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73289518]\n",
            " [ 0.04394414]\n",
            " [-0.22827793]]\n",
            "[[-0.14096441]\n",
            " [ 0.15973099]\n",
            " [ 1.25003907]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "98 / 201\n",
            "Pose estimation:99\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72711708]\n",
            " [ 0.04650146]\n",
            " [-0.23823448]]\n",
            "[[-0.13883685]\n",
            " [ 0.15847716]\n",
            " [ 1.24947424]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "99 / 201\n",
            "Pose estimation:100\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.74023349]\n",
            " [ 0.03824516]\n",
            " [-0.22070791]]\n",
            "[[-0.12591225]\n",
            " [ 0.15686915]\n",
            " [ 1.28748902]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "100 / 201\n",
            "Pose estimation:101\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73012028]\n",
            " [ 0.04479067]\n",
            " [-0.23732889]]\n",
            "[[-0.13611269]\n",
            " [ 0.16426989]\n",
            " [ 1.21908973]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "101 / 201\n",
            "Pose estimation:102\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72632089]\n",
            " [ 0.01110409]\n",
            " [-0.23173138]]\n",
            "[[-0.11813762]\n",
            " [ 0.16330863]\n",
            " [ 1.24692313]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "102 / 201\n",
            "Pose estimation:103\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72691055]\n",
            " [ 0.03204225]\n",
            " [-0.18418919]]\n",
            "[[-0.1142925 ]\n",
            " [ 0.15319912]\n",
            " [ 1.23721828]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "103 / 201\n",
            "Pose estimation:104\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72653913]\n",
            " [ 0.03215335]\n",
            " [-0.1876873 ]]\n",
            "[[-0.1178227]\n",
            " [ 0.158467 ]\n",
            " [ 1.2042773]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "104 / 201\n",
            "Pose estimation:105\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73707971]\n",
            " [ 0.04214001]\n",
            " [-0.22389476]]\n",
            "[[-0.12486231]\n",
            " [ 0.16154483]\n",
            " [ 1.21725959]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "105 / 201\n",
            "Pose estimation:106\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.74188748]\n",
            " [ 0.03607997]\n",
            " [-0.21335752]]\n",
            "[[-0.11396179]\n",
            " [ 0.15819142]\n",
            " [ 1.25049438]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "106 / 201\n",
            "Pose estimation:107\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.74281994]\n",
            " [ 0.03411528]\n",
            " [-0.20598481]]\n",
            "[[-0.10815116]\n",
            " [ 0.15851651]\n",
            " [ 1.24799116]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "107 / 201\n",
            "Pose estimation:108\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73100846]\n",
            " [ 0.07874371]\n",
            " [-0.15088222]]\n",
            "[[-0.11413889]\n",
            " [ 0.13951772]\n",
            " [ 1.2402145 ]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "108 / 201\n",
            "Pose estimation:109\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.74756527]\n",
            " [ 0.06108649]\n",
            " [-0.18148652]]\n",
            "[[-0.11758423]\n",
            " [ 0.14979508]\n",
            " [ 1.24594983]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "109 / 201\n",
            "Pose estimation:110\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.74801663]\n",
            " [ 0.05246946]\n",
            " [-0.19459649]]\n",
            "[[-0.12237734]\n",
            " [ 0.15420654]\n",
            " [ 1.23297141]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "110 / 201\n",
            "Pose estimation:111\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.75056915]\n",
            " [ 0.04111687]\n",
            " [-0.1989436 ]]\n",
            "[[-0.12374694]\n",
            " [ 0.15695166]\n",
            " [ 1.23491007]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "111 / 201\n",
            "Pose estimation:112\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.74584404]\n",
            " [ 0.03294102]\n",
            " [-0.19982162]]\n",
            "[[-0.12570157]\n",
            " [ 0.15664662]\n",
            " [ 1.23594387]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "112 / 201\n",
            "Pose estimation:113\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "113 / 201\n",
            "Pose estimation:114\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "114 / 201\n",
            "Pose estimation:115\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "115 / 201\n",
            "Pose estimation:116\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "116 / 201\n",
            "Pose estimation:117\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "117 / 201\n",
            "Pose estimation:118\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "118 / 201\n",
            "Pose estimation:119\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "119 / 201\n",
            "Pose estimation:120\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "120 / 201\n",
            "Pose estimation:121\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "121 / 201\n",
            "Pose estimation:122\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.75658499]\n",
            " [ 0.02427307]\n",
            " [-0.09955095]]\n",
            "[[-0.11369427]\n",
            " [ 0.15521465]\n",
            " [ 1.22731902]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "122 / 201\n",
            "Pose estimation:123\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.75039756]\n",
            " [ 0.03733119]\n",
            " [-0.09453199]]\n",
            "[[-0.11735414]\n",
            " [ 0.15148841]\n",
            " [ 1.2221209 ]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "123 / 201\n",
            "Pose estimation:124\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.75133079]\n",
            " [ 0.00529235]\n",
            " [-0.13060596]]\n",
            "[[-0.11255384]\n",
            " [ 0.16312238]\n",
            " [ 1.22765739]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "124 / 201\n",
            "Pose estimation:125\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.74059467e+00]\n",
            " [ 2.08034389e-03]\n",
            " [-1.12702751e-01]]\n",
            "[[-0.10501705]\n",
            " [ 0.15990608]\n",
            " [ 1.21895731]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "125 / 201\n",
            "Pose estimation:126\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73956754e+00]\n",
            " [-5.29574463e-04]\n",
            " [-9.77384627e-02]]\n",
            "[[-0.10226195]\n",
            " [ 0.15944955]\n",
            " [ 1.21438605]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "126 / 201\n",
            "Pose estimation:127\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73946288]\n",
            " [ 0.00586771]\n",
            " [-0.1237233 ]]\n",
            "[[-0.10759068]\n",
            " [ 0.16374522]\n",
            " [ 1.22203596]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "127 / 201\n",
            "Pose estimation:128\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73903281]\n",
            " [ 0.03085927]\n",
            " [-0.09208049]]\n",
            "[[-0.11106553]\n",
            " [ 0.15863562]\n",
            " [ 1.21519479]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "128 / 201\n",
            "Pose estimation:129\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73354759e+00]\n",
            " [-2.44375598e-03]\n",
            " [-9.27801648e-02]]\n",
            "[[-0.10103995]\n",
            " [ 0.16676667]\n",
            " [ 1.21117806]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "129 / 201\n",
            "Pose estimation:130\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73672540e+00]\n",
            " [ 3.78881425e-04]\n",
            " [-1.08017047e-01]]\n",
            "[[-0.10381662]\n",
            " [ 0.17219186]\n",
            " [ 1.21714281]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "130 / 201\n",
            "Pose estimation:131\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72946444]\n",
            " [-0.00350604]\n",
            " [-0.09192134]]\n",
            "[[-0.10053412]\n",
            " [ 0.17012742]\n",
            " [ 1.21024141]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "131 / 201\n",
            "Pose estimation:132\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73016649]\n",
            " [ 0.02748893]\n",
            " [-0.08715388]]\n",
            "[[-0.10943545]\n",
            " [ 0.16544639]\n",
            " [ 1.21100327]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "132 / 201\n",
            "Pose estimation:133\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72504316]\n",
            " [ 0.02802321]\n",
            " [-0.08567825]]\n",
            "[[-0.10941947]\n",
            " [ 0.16373187]\n",
            " [ 1.20914719]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "133 / 201\n",
            "Pose estimation:134\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.7225326 ]\n",
            " [ 0.02657589]\n",
            " [-0.07947853]]\n",
            "[[-0.10813942]\n",
            " [ 0.1629984 ]\n",
            " [ 1.20662905]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "134 / 201\n",
            "Pose estimation:135\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72241306]\n",
            " [ 0.0258041 ]\n",
            " [-0.07737129]]\n",
            "[[-0.10761661]\n",
            " [ 0.16303366]\n",
            " [ 1.20595379]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "135 / 201\n",
            "Pose estimation:136\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72619456]\n",
            " [ 0.02056955]\n",
            " [-0.05956787]]\n",
            "[[-0.09602665]\n",
            " [ 0.15946657]\n",
            " [ 1.23702379]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "136 / 201\n",
            "Pose estimation:137\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72143405]\n",
            " [ 0.02243342]\n",
            " [-0.06826897]]\n",
            "[[-0.10532612]\n",
            " [ 0.16304878]\n",
            " [ 1.20296197]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "137 / 201\n",
            "Pose estimation:138\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.71956103]\n",
            " [ 0.02100665]\n",
            " [-0.06324596]]\n",
            "[[-0.10422088]\n",
            " [ 0.16258843]\n",
            " [ 1.20072776]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "138 / 201\n",
            "Pose estimation:139\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72340097]\n",
            " [ 0.01745868]\n",
            " [-0.05401686]]\n",
            "[[-0.09889335]\n",
            " [ 0.15911439]\n",
            " [ 1.23413375]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "139 / 201\n",
            "Pose estimation:140\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72924809]\n",
            " [-0.01213468]\n",
            " [-0.07831505]]\n",
            "[[-0.09321309]\n",
            " [ 0.17088397]\n",
            " [ 1.24018239]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "140 / 201\n",
            "Pose estimation:141\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73423392]\n",
            " [-0.01530671]\n",
            " [-0.06804417]]\n",
            "[[-0.09083912]\n",
            " [ 0.17260076]\n",
            " [ 1.23850301]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "141 / 201\n",
            "Pose estimation:142\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73219263]\n",
            " [-0.01761947]\n",
            " [-0.05758125]]\n",
            "[[-0.08876075]\n",
            " [ 0.17210664]\n",
            " [ 1.23428138]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "142 / 201\n",
            "Pose estimation:143\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73028599]\n",
            " [ 0.01825144]\n",
            " [-0.06084428]]\n",
            "[[-0.10012107]\n",
            " [ 0.16589334]\n",
            " [ 1.23794723]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "143 / 201\n",
            "Pose estimation:144\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72068715]\n",
            " [-0.01428645]\n",
            " [-0.06187763]]\n",
            "[[-0.09318254]\n",
            " [ 0.172936  ]\n",
            " [ 1.19817779]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "144 / 201\n",
            "Pose estimation:145\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73076863]\n",
            " [ 0.01528349]\n",
            " [-0.04986159]]\n",
            "[[-0.09322477]\n",
            " [ 0.16629056]\n",
            " [ 1.23425049]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "145 / 201\n",
            "Pose estimation:146\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73052336]\n",
            " [ 0.01532585]\n",
            " [-0.05092268]]\n",
            "[[-0.09341402]\n",
            " [ 0.16629166]\n",
            " [ 1.2342427 ]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "146 / 201\n",
            "Pose estimation:147\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73038391]\n",
            " [ 0.01814261]\n",
            " [-0.05860681]]\n",
            "[[-0.09520954]\n",
            " [ 0.16584479]\n",
            " [ 1.23740217]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "147 / 201\n",
            "Pose estimation:148\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72110743]\n",
            " [-0.01106158]\n",
            " [-0.07358681]]\n",
            "[[-0.09120225]\n",
            " [ 0.17277205]\n",
            " [ 1.20277624]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "148 / 201\n",
            "Pose estimation:149\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72810144]\n",
            " [ 0.01541423]\n",
            " [-0.04960688]]\n",
            "[[-0.08855954]\n",
            " [ 0.1653385 ]\n",
            " [ 1.23411833]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "149 / 201\n",
            "Pose estimation:150\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72780815]\n",
            " [ 0.01778427]\n",
            " [-0.05556304]]\n",
            "[[-0.09304763]\n",
            " [ 0.16527055]\n",
            " [ 1.20216561]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "150 / 201\n",
            "Pose estimation:151\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73011365]\n",
            " [ 0.0179527 ]\n",
            " [-0.05459065]]\n",
            "[[-0.08545522]\n",
            " [ 0.16103913]\n",
            " [ 1.23635785]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "151 / 201\n",
            "Pose estimation:152\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72998106]\n",
            " [ 0.04364521]\n",
            " [-0.02197842]]\n",
            "[[-0.08403832]\n",
            " [ 0.15156544]\n",
            " [ 1.22928086]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "152 / 201\n",
            "Pose estimation:153\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72310191]\n",
            " [ 0.05092824]\n",
            " [-0.04449156]]\n",
            "[[-0.09257914]\n",
            " [ 0.14920764]\n",
            " [ 1.20006342]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "153 / 201\n",
            "Pose estimation:154\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73223081]\n",
            " [ 0.02251154]\n",
            " [-0.06395104]]\n",
            "[[-0.08220256]\n",
            " [ 0.1569393 ]\n",
            " [ 1.20655557]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "154 / 201\n",
            "Pose estimation:155\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73556591]\n",
            " [ 0.04944934]\n",
            " [-0.03517156]]\n",
            "[[-0.07843213]\n",
            " [ 0.14324927]\n",
            " [ 1.23586516]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "155 / 201\n",
            "Pose estimation:156\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73554001]\n",
            " [ 0.05110274]\n",
            " [-0.04195879]]\n",
            "[[-0.0749634 ]\n",
            " [ 0.13828165]\n",
            " [ 1.23993925]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "156 / 201\n",
            "Pose estimation:157\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73911221]\n",
            " [ 0.04549275]\n",
            " [-0.02404012]]\n",
            "[[-0.06197859]\n",
            " [ 0.14017597]\n",
            " [ 1.23389252]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "157 / 201\n",
            "Pose estimation:158\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.73968247]\n",
            " [ 0.01456848]\n",
            " [-0.05547227]]\n",
            "[[-0.05285973]\n",
            " [ 0.15257733]\n",
            " [ 1.20190869]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "158 / 201\n",
            "Pose estimation:159\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.74474596]\n",
            " [ 0.0330476 ]\n",
            " [-0.02655585]]\n",
            "[[-0.04362426]\n",
            " [ 0.1460155 ]\n",
            " [ 1.20457145]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "159 / 201\n",
            "Pose estimation:160\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "160 / 201\n",
            "Pose estimation:161\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "161 / 201\n",
            "Pose estimation:162\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "162 / 201\n",
            "Pose estimation:163\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "163 / 201\n",
            "Pose estimation:164\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "164 / 201\n",
            "Pose estimation:165\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "165 / 201\n",
            "Pose estimation:166\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "166 / 201\n",
            "Pose estimation:167\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "167 / 201\n",
            "Pose estimation:168\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "168 / 201\n",
            "Pose estimation:169\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "169 / 201\n",
            "Pose estimation:170\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "170 / 201\n",
            "Pose estimation:171\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "171 / 201\n",
            "Pose estimation:172\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "172 / 201\n",
            "Pose estimation:173\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "173 / 201\n",
            "Pose estimation:174\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "174 / 201\n",
            "Pose estimation:175\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "175 / 201\n",
            "Pose estimation:176\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "done\n",
            "(270, 480, 3)\n",
            "176 / 201\n",
            "Pose estimation:177\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.72446041]\n",
            " [ 0.06973077]\n",
            " [-0.10494093]]\n",
            "[[-0.01049828]\n",
            " [ 0.13838664]\n",
            " [ 1.22840974]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "177 / 201\n",
            "Pose estimation:178\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.71012715]\n",
            " [ 0.11003545]\n",
            " [-0.08793723]]\n",
            "[[-0.00963445]\n",
            " [ 0.11992484]\n",
            " [ 1.25913085]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "178 / 201\n",
            "Pose estimation:179\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.70401187]\n",
            " [ 0.10944613]\n",
            " [-0.08683307]]\n",
            "[[-0.00915493]\n",
            " [ 0.12262801]\n",
            " [ 1.25774773]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "179 / 201\n",
            "Pose estimation:180\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69680266]\n",
            " [ 0.10748451]\n",
            " [-0.08373498]]\n",
            "[[-0.00341882]\n",
            " [ 0.12521812]\n",
            " [ 1.255303  ]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "180 / 201\n",
            "Pose estimation:181\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.6955807 ]\n",
            " [ 0.08027498]\n",
            " [-0.11798511]]\n",
            "[[0.00541111]\n",
            " [0.13476414]\n",
            " [1.26641687]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "181 / 201\n",
            "Pose estimation:182\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.69336414]\n",
            " [ 0.08093828]\n",
            " [-0.11771305]]\n",
            "[[0.00997797]\n",
            " [0.1384704 ]\n",
            " [1.26626444]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "182 / 201\n",
            "Pose estimation:183\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.68753375]\n",
            " [ 0.07487956]\n",
            " [-0.10275614]]\n",
            "[[0.01344812]\n",
            " [0.14205071]\n",
            " [1.25850362]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "183 / 201\n",
            "Pose estimation:184\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.6706978 ]\n",
            " [ 0.05844966]\n",
            " [-0.15409689]]\n",
            "[[0.00614134]\n",
            " [0.15489662]\n",
            " [1.23547402]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "184 / 201\n",
            "Pose estimation:185\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.67382713]\n",
            " [ 0.07264124]\n",
            " [-0.10182839]]\n",
            "[[0.03036903]\n",
            " [0.14710417]\n",
            " [1.29290868]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "185 / 201\n",
            "Pose estimation:186\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.66527965]\n",
            " [ 0.07131899]\n",
            " [-0.0924553 ]]\n",
            "[[0.02493554]\n",
            " [0.15388624]\n",
            " [1.24822145]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "186 / 201\n",
            "Pose estimation:187\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.66840496]\n",
            " [ 0.06919222]\n",
            " [-0.09463481]]\n",
            "[[0.03022455]\n",
            " [0.15533204]\n",
            " [1.25097337]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "187 / 201\n",
            "Pose estimation:188\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.66178817]\n",
            " [ 0.06820833]\n",
            " [-0.09148206]]\n",
            "[[0.03565998]\n",
            " [0.15785894]\n",
            " [1.24845549]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "188 / 201\n",
            "Pose estimation:189\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.66231543]\n",
            " [ 0.0403401 ]\n",
            " [-0.11942714]]\n",
            "[[0.05251281]\n",
            " [0.16344986]\n",
            " [1.29518852]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "189 / 201\n",
            "Pose estimation:190\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.65995294]\n",
            " [ 0.03707681]\n",
            " [-0.10531759]]\n",
            "[[0.05953544]\n",
            " [0.16764482]\n",
            " [1.28751229]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "190 / 201\n",
            "Pose estimation:191\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.65670595]\n",
            " [ 0.03667517]\n",
            " [-0.10745022]]\n",
            "[[0.05255571]\n",
            " [0.1711499 ]\n",
            " [1.25088961]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "191 / 201\n",
            "Pose estimation:192\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.65733526]\n",
            " [ 0.04090921]\n",
            " [-0.12183116]]\n",
            "[[0.06173774]\n",
            " [0.16654767]\n",
            " [1.29545423]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "192 / 201\n",
            "Pose estimation:193\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.63012399]\n",
            " [ 0.08024597]\n",
            " [-0.10932692]]\n",
            "[[0.04775173]\n",
            " [0.15050614]\n",
            " [1.29001554]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "193 / 201\n",
            "Pose estimation:194\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.61806712]\n",
            " [ 0.0683866 ]\n",
            " [-0.09560928]]\n",
            "[[0.0420937 ]\n",
            " [0.15281776]\n",
            " [1.2478461 ]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "194 / 201\n",
            "Pose estimation:195\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.63217917]\n",
            " [ 0.03340643]\n",
            " [-0.09862121]]\n",
            "[[0.04190185]\n",
            " [0.17054415]\n",
            " [1.22700736]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "195 / 201\n",
            "Pose estimation:196\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.63305825]\n",
            " [ 0.05657336]\n",
            " [-0.06923202]]\n",
            "[[0.05498395]\n",
            " [0.15624763]\n",
            " [1.30458931]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "196 / 201\n",
            "Pose estimation:197\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.61431053]\n",
            " [ 0.06583451]\n",
            " [-0.08120905]]\n",
            "[[0.03548188]\n",
            " [0.15600321]\n",
            " [1.24143458]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "197 / 201\n",
            "Pose estimation:198\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.64070718e+00]\n",
            " [-1.11806060e-03]\n",
            " [-6.97629432e-02]]\n",
            "[[0.05696079]\n",
            " [0.18114794]\n",
            " [1.24850561]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "198 / 201\n",
            "Pose estimation:199\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.64924133]\n",
            " [ 0.02542235]\n",
            " [-0.07673558]]\n",
            "[[0.04400549]\n",
            " [0.18174099]\n",
            " [1.24510621]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "199 / 201\n",
            "Pose estimation:200\n",
            "data ready\n",
            "testX.shape (26656, 32, 32, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "testX.shape (1, 64, 64, 1)\n",
            "[[ 2.65718623]\n",
            " [ 0.06791589]\n",
            " [-0.07498322]]\n",
            "[[0.03518809]\n",
            " [0.16999874]\n",
            " [1.27634425]]\n",
            "done\n",
            "(270, 480, 3)\n",
            "200 / 201\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    }
  ]
}